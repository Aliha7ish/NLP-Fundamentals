{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3093e0ea-33b3-4ce8-b075-598e39e99e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62cb6aac-6d4d-4b6d-aeaa-6519c4aa1af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Data Type ------\n",
      "data: <class 'str'>\n",
      "------ ------\n",
      "------ # Letter ------\n",
      "# letters: 1848\n",
      "------ ------\n",
      "\n",
      "------ ------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Reading Eliot\\'s \"Four Quartets\" in memory of Theo Angelopoulos. \"In my end is my beginning.\"\\nScrew the law we just some mother fukkin kids.\\n#YouWannaImpressMe respect Justin Bieber and I\\'ll be impressed alright ;)\\nJust because it\\'s 4:20 somewhere\\nHappy birthday\\nSetting goals includes determining you'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "------ ------\n"
     ]
    }
   ],
   "source": [
    "# Loading and reading data\n",
    "# data_path = \"en_US_twitter.txt\"\n",
    "data_path = \"test_text.txt\"\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Exploring data\n",
    "print(\"------ Data Type ------\")\n",
    "print(f\"data: {type(data)}\")\n",
    "print(\"------ ------\")\n",
    "print(\"------ # Letter ------\")\n",
    "print(f\"# letters: {len(data)}\")\n",
    "print(\"------ ------\")\n",
    "print()\n",
    "print(\"------ ------\")\n",
    "print(display(data[:300]))\n",
    "print(\"------ ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f4c45f4-177e-4bbe-8886-211e08588e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# 1- splitting data into sentences\n",
    "def split_data_to_sentences(data):\n",
    "    sents = [sent.strip() for sent in data.split(\"\\n\") if len(sent) > 0]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61644f82-c99b-41f0-aa9b-8b36d2eefd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there', 'welcome to the assignment.', 'of this', 'week.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = split_data_to_sentences(\"Hello there\\n welcome to the assignment.\\n of this \\n week. \\n\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7cceda-2ed7-4b13-9d88-6d5b6da7cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- splitting sentences into tokens\n",
    "def tokenize_sents(sentences):\n",
    "    tokens_l = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.lower()\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tokens_l.append(tokens)\n",
    "        \n",
    "    return tokens_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a70e971-4131-40a6-ad01-b5798c858b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'there'],\n",
       " ['welcome', 'to', 'the', 'assignment', '.'],\n",
       " ['of', 'this'],\n",
       " ['week', '.']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_l = tokenize_sents(x)\n",
    "tokens_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "48dc2dda-950d-4823-a705-81ad2f49165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- applying preprocessing functions to the original data\n",
    "def tokenize_data(data):\n",
    "    sents = split_data_to_sentences(data)\n",
    "    tokens_l = tokenize_sents(sents)\n",
    "    return tokens_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "937b368f-046b-44fe-b41d-c580d9468be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'there'],\n",
       " ['welcome', 'to', 'the', 'assignment', '.'],\n",
       " ['of', 'this'],\n",
       " ['week', '.']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_data(\"Hello there\\n welcome to the assignment.\\n of this \\n week. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db0f0903-cd6f-4bce-b2bc-3205f78a12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- splitting data into train and test sets\n",
    "tokenized_data = tokenize_data(data)\n",
    "# setting seed for consistincy\n",
    "random.seed(321)\n",
    "\n",
    "# train size = 80%\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "# splitting data into train and test\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5aba31bd-24dc-4b5e-975c-3ccd5434e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training data 22\n",
      "length of test data 6\n",
      "\n",
      "Train: ['reading', 'eliot', \"'s\", '``', 'four', 'quartets', \"''\", 'in', 'memory', 'of', 'theo', 'angelopoulos', '.', '``', 'in', 'my', 'end', 'is', 'my', 'beginning', '.', \"''\"]\n",
      "Test: ['not', 'gon', 'na', 'lie', 'i', \"'m\", 'lovin', 'this', 'rental', 'ford', 'focus', 'hatchback']\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of training data {len(train_data)}\")\n",
    "print(f\"length of test data {len(test_data)}\")\n",
    "print()\n",
    "print(f\"Train: {train_data[0]}\")\n",
    "print(f\"Test: {test_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3686b6cd-dd97-469e-8347-95a4828661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- get words frequency\n",
    "def count_words(tokenized_l):\n",
    "    # initialize counts dict\n",
    "    counts = {token: 0 for sent in tokenized_l for token in sent}\n",
    "    # loop through each sentence withing the list\n",
    "    for sent in tokenized_l:\n",
    "        # lopp through each token withing the sentence\n",
    "        for token in sent:\n",
    "            if token in counts:\n",
    "                counts[token] +=1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60179138-a7f9-490c-99b8-65d2e1dd46d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': 1,\n",
       " 'i': 3,\n",
       " 'am': 2,\n",
       " 'ali': 2,\n",
       " 'welcome': 1,\n",
       " 'to': 1,\n",
       " 'my': 1,\n",
       " 'channel': 2,\n",
       " 'love': 1,\n",
       " 'python': 2,\n",
       " '.': 2,\n",
       " 'coding': 1,\n",
       " 'is': 1,\n",
       " 'amazing': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [[\"hi\", \"i\", \"am\"], [\"ali\", \"am\", \"i\"], [\"welcome\", \"to\", \"my\", \"channel\", \"ali\", \"channel\"], [\"i\", \"love\", \"python\", \".\"], [\"python\", \"coding\", \"is\", \"amazing\", \".\"]]\n",
    "test_counts = count_words(s)\n",
    "test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1a81b77-6de4-4ce3-a204-1c1407db400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6- Handling OOV by keeping words that satisify threshold and others will be set to <unk> token\n",
    "def most_freq_words_to_keep(counts_dict, threshold):\n",
    "    # store words to keep\n",
    "    kept_words = [w for w, v in counts_dict.items() if v >= threshold]\n",
    "            \n",
    "    return kept_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5bcd305a-5aa9-4f24-affa-e2689a52985e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'ali', 'channel', 'python', '.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_v = most_freq_words_to_keep(test_counts, 2)\n",
    "close_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c32a791-8ad4-4a35-8ced-ec59802af631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7- Replace non frequent words in each sentence with <unk> token\n",
    "def replace_words_with_tokens(close_v, tokenized_l):\n",
    "    # set unk token\n",
    "    unk_token = \"<unk>\"\n",
    "    # loop through indices of outer list\n",
    "    for i in range(len(tokenized_l)):\n",
    "        # loop through inner list indices\n",
    "        for j in range(len(tokenized_l[i])):\n",
    "            # if token in close vocab keep it unchanged\n",
    "            if tokenized_l[i][j] in close_v:\n",
    "                continue\n",
    "            # else replace it with <unk>\n",
    "            else:\n",
    "                tokenized_l[i][j] = unk_token\n",
    "                \n",
    "    return tokenized_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2bcd749-45d2-4843-a38f-ec29c3cddc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 'i', 'am'],\n",
       " ['ali', 'am', 'i'],\n",
       " ['<unk>', '<unk>', '<unk>', 'channel', 'ali', 'channel'],\n",
       " ['i', '<unk>', 'python', '.'],\n",
       " ['python', '<unk>', '<unk>', '<unk>', '.']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_l_oov = replace_words_with_tokens(close_v, s)\n",
    "tokenized_l_oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c1e13cc9-ba76-40c1-96f2-739855f63324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8- preprocessing entire data by storing most frequent words from training set and replace words \n",
    "# in both training and test set by <unk> token\n",
    "def preprocess_tokenized_data(train_tokenized, test_tokenized, threshold):\n",
    "    # first build the frequency matrix from train data\n",
    "    counts_dict = count_words(train_tokenized)\n",
    "    # make close_vocab using frequency matrix\n",
    "    close_vocab = most_freq_words_to_keep(counts_dict, threshold)\n",
    "    # preprocess both train and test tokenized sentences to handle OOV\n",
    "    train_tokenized_pre = replace_words_with_tokens(close_vocab, train_tokenized)\n",
    "    test_tokenized_pre = replace_words_with_tokens(close_vocab, test_tokenized)\n",
    "    # return both preprocessed sentences and the close vocab\n",
    "    return train_tokenized_pre, test_tokenized_pre, close_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b51581df-dda4-40ad-a199-e331fc319090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', 'are', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']\n"
     ]
    }
   ],
   "source": [
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_tokenized_data(tmp_train, tmp_test, 1)\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c647ed88-3768-4ad3-9255-96ab3ae2af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9- n-grams frequency dictionary\n",
    "def build_n_gram_word_frequency(list_of_tokenized_sentences, n):\n",
    "    sos_token = \"<s>\"\n",
    "    eos_token = \"<e>\"\n",
    "    \n",
    "    # for each sentence \n",
    "    # for i in range(len(list_of_tokenized_sentences)):\n",
    "    #     list_of_tokenized_sentences[i] = deque(list_of_tokenized_sentences[i])\n",
    "    #     if n > 1:\n",
    "    #         # add n-1 sos tokens to the start of the sentence\n",
    "    #         for j in range(n - 1):\n",
    "    #             list_of_tokenized_sentences[i].appendleft(sos_token)\n",
    "    #     else:\n",
    "    #         list_of_tokenized_sentences[i].appendleft(sos_token)\n",
    "\n",
    "    #     list_of_tokenized_sentences[i] = list(list_of_tokenized_sentences[i])\n",
    "         \n",
    "    \n",
    "    bag_counter = {}\n",
    "\n",
    "    # loop through the entire list\n",
    "    for sentence in list_of_tokenized_sentences:\n",
    "        # adding eos token to the sentence\n",
    "        sentence = [sos_token] * n + sentence + [eos_token]\n",
    "        # convert sentence to tuple\n",
    "        sentence = tuple(sentence)\n",
    "        # looping through length of the sentence-n+1  \n",
    "        for i in range(len(sentence)-n+1):\n",
    "            # n-gram sequence starts from i to i+n\n",
    "            n_gram = sentence[i: i + n]\n",
    "            # check if the n-gram is in the dictionary\n",
    "            if n_gram not in bag_counter:\n",
    "                # initialize it with 1 if it is the first time occuring\n",
    "                bag_counter[n_gram] = 1\n",
    "            else:\n",
    "                # increment the frequency by 1\n",
    "                bag_counter[n_gram] += 1\n",
    "                \n",
    "    return bag_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e1b18f61-ce7a-48a9-89c7-c091762117a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n",
      "['i', 'like', 'a', 'cat']\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(build_n_gram_word_frequency(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(build_n_gram_word_frequency(sentences, 2))\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "fbed6d8b-2fbe-498a-8b4a-3b3c507395e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram frequency: {('<s>',): 4, ('sky',): 2, ('is',): 2, ('blue',): 1, ('.',): 1, ('<e>',): 4, ('leaves',): 2, ('are',): 2, ('green',): 2, ('red',): 1}\n",
      "Bigram frequency: {('<s>', '<s>'): 4, ('<s>', 'sky'): 2, ('sky', 'is'): 2, ('is', 'blue'): 1, ('blue', '.'): 1, ('.', '<e>'): 1, ('<s>', 'leaves'): 2, ('leaves', 'are'): 2, ('are', 'green'): 1, ('green', '<e>'): 2, ('are', 'red'): 1, ('red', '<e>'): 1, ('is', 'green'): 1}\n",
      "Trigram frequency: {('<s>', '<s>', '<s>'): 4, ('<s>', '<s>', 'sky'): 2, ('<s>', 'sky', 'is'): 2, ('sky', 'is', 'blue'): 1, ('is', 'blue', '.'): 1, ('blue', '.', '<e>'): 1, ('<s>', '<s>', 'leaves'): 2, ('<s>', 'leaves', 'are'): 2, ('leaves', 'are', 'green'): 1, ('are', 'green', '<e>'): 1, ('leaves', 'are', 'red'): 1, ('are', 'red', '<e>'): 1, ('sky', 'is', 'green'): 1, ('is', 'green', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "tmp_train = [['sky', 'is', 'blue', '.'],\n",
    "     ['leaves', 'are', 'green'],\n",
    "    ['leaves', 'are', 'red'],\n",
    "    ['sky', 'is', 'green']]\n",
    "# unigram\n",
    "unigram = build_n_gram_word_frequency(tmp_train, 1)\n",
    "print(f\"Unigram frequency: {unigram}\")\n",
    "\n",
    "bigram = build_n_gram_word_frequency(tmp_train, 2)\n",
    "print(f\"Bigram frequency: {bigram}\")\n",
    "\n",
    "trigram = build_n_gram_word_frequency(tmp_train, 3)\n",
    "print(f\"Trigram frequency: {trigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4e3eec40-a9b8-4912-9dbf-06dfe6623dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10- Building n-gram probaility matrix using k-smoothing (C(prev n-gram + word) / C(prev n-gram))\n",
    "# to handle non-occuring n-grams\n",
    "def build_n_gram_probability_with_k_smoothing(word, previous_n_gram, n_gram_counts, n_plus_one_gram_counts, vocab, k):\n",
    "    # length of vocabulary\n",
    "    v = len(vocab)\n",
    "    # converting previous_n_gram to tuple\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    # now we check if the previous n-gram in the n_gram_counts to take its value (denominator)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "    # add the word to the previous n-gram\n",
    "    prev_n_gram_plus_word = previous_n_gram + (word, )\n",
    "    # check if the n-gram plus word is in the n_gram_plus_word_counts dictionary\n",
    "    prev_n_gram_plus_word_count = n_plus_one_gram_counts.get(prev_n_gram_plus_word, 0)\n",
    "    # Now the formula is dividing the count of previous n-gram + word + k by the previous_n_gram_count + kXv\n",
    "    numerator = prev_n_gram_plus_word_count + k\n",
    "    denominator = previous_n_gram_count + (k * v)\n",
    "    # getting the conditional probability\n",
    "    cond_prob = numerator / denominator\n",
    "    # return probability\n",
    "    return cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a9e95eb9-f597-433c-aeb6-f96e652fe6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case\n",
    "sentences = [[\"i\", \"love\", \"cats\", \"and\", \"dogs\", \".\"], [\"cats\", \"are\", \"cute\", \"animals\", \".\"]]\n",
    "vocab = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = build_n_gram_word_frequency(sentences, 1)\n",
    "bigram_counts = build_n_gram_word_frequency(sentences, 2)\n",
    "\n",
    "build_n_gram_probability_with_k_smoothing(\"cats\", \"are\", unigram_counts, bigram_counts, vocab, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2cb0160c-3a7c-4ee6-a619-7ed9e50156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11- now we will use the previous function to estimate the probabilities of all words in the vocabulary\n",
    "def estimate_probabilities(prev_n_gram, n_gram_counts, n_plus_one_gram_counts, vocab, k):\n",
    "    # add end of sentence token and unknown token to the vocab.\n",
    "    # execlude start of the sentence token as it shouldn't be the next word\n",
    "    vocab = vocab + [\"<unk>\", \"<e>\"]\n",
    "    # estimate probability for each word in the vocab\n",
    "    n_gram_probs = {word: build_n_gram_probability_with_k_smoothing(word, prev_n_gram, n_gram_counts, n_plus_one_gram_counts, vocab, k)\n",
    "                    for word in vocab}\n",
    "    \n",
    "    return n_gram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d40df9e8-d52b-4a0d-be01-2262b573e66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'i': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " 'dog': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = build_n_gram_word_frequency(sentences, 1)\n",
    "bigram_counts = build_n_gram_word_frequency(sentences, 2)\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "da8a64d0-0f0a-42ff-a5d6-8d593623914a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'is': 0.09090909090909091,\n",
       " 'i': 0.18181818181818182,\n",
       " 'this': 0.18181818181818182,\n",
       " 'cat': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional test\n",
    "trigram_counts = build_n_gram_word_frequency(sentences, 3)\n",
    "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "fc78ba65-3682-4e46-a21e-0b263496ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "    # add <e> <unk> to the vocabulary\n",
    "    # <s> is omitted since it should not appear as the next word\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtain unique n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ace20c2c-3d64-47e2-8818-40e03023e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>is</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           a  like   is    i  this  cat  dog  <e>  <unk>\n",
       "(like,)  2.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(i,)     0.0   1.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>,)   0.0   0.0  0.0  1.0   1.0  0.0  0.0  0.0    0.0\n",
       "(this,)  0.0   0.0  0.0  0.0   0.0  0.0  1.0  0.0    0.0\n",
       "(is,)    0.0   1.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(cat,)   0.0   0.0  0.0  0.0   0.0  0.0  0.0  2.0    0.0\n",
       "(a,)     0.0   0.0  0.0  0.0   0.0  2.0  0.0  0.0    0.0\n",
       "(dog,)   0.0   0.0  1.0  0.0   0.0  0.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = build_n_gram_word_frequency(sentences, 2)\n",
    "\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5376c5ce-749a-4470-9db6-f4ea0d4d2f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>is</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               a  like   is    i  this  cat  dog  <e>  <unk>\n",
       "(i, like)    1.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>, i)     0.0   1.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(this, dog)  0.0   0.0  1.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>, <s>)   0.0   0.0  0.0  1.0   1.0  0.0  0.0  0.0    0.0\n",
       "(dog, is)    0.0   1.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(<s>, this)  0.0   0.0  0.0  0.0   0.0  0.0  1.0  0.0    0.0\n",
       "(is, like)   1.0   0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0\n",
       "(a, cat)     0.0   0.0  0.0  0.0   0.0  0.0  0.0  2.0    0.0\n",
       "(like, a)    0.0   0.0  0.0  0.0   0.0  2.0  0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_counts = build_n_gram_word_frequency(sentences, 3)\n",
    "\n",
    "print('trigram counts')\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1b789ff2-8be4-4a56-b98f-58321ee6547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "111a2db8-4ad3-420e-b475-7aab066d64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>is</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                a      like        is         i      this       cat       dog  \\\n",
       "(like,)  0.272727  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(i,)     0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>,)   0.090909  0.090909  0.090909  0.181818  0.181818  0.090909  0.090909   \n",
       "(this,)  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "(is,)    0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(a,)     0.090909  0.090909  0.090909  0.090909  0.090909  0.272727  0.090909   \n",
       "(dog,)   0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(like,)  0.090909  0.090909  \n",
       "(i,)     0.100000  0.100000  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(this,)  0.100000  0.100000  \n",
       "(is,)    0.100000  0.100000  \n",
       "(cat,)   0.272727  0.090909  \n",
       "(a,)     0.090909  0.090909  \n",
       "(dog,)   0.100000  0.100000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = build_n_gram_word_frequency(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "2f41f9ee-ab52-4637-b605-935dfe86f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>is</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    a      like        is         i      this       cat  \\\n",
       "(i, like)    0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, i)     0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(this, dog)  0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  0.181818  0.181818  0.090909   \n",
       "(dog, is)    0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, this)  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(is, like)   0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(like, a)    0.090909  0.090909  0.090909  0.090909  0.090909  0.272727   \n",
       "\n",
       "                  dog       <e>     <unk>  \n",
       "(i, like)    0.100000  0.100000  0.100000  \n",
       "(<s>, i)     0.100000  0.100000  0.100000  \n",
       "(this, dog)  0.100000  0.100000  0.100000  \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  \n",
       "(dog, is)    0.100000  0.100000  0.100000  \n",
       "(<s>, this)  0.200000  0.100000  0.100000  \n",
       "(is, like)   0.100000  0.100000  0.100000  \n",
       "(a, cat)     0.090909  0.272727  0.090909  \n",
       "(like, a)    0.090909  0.090909  0.090909  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trigram_counts = build_n_gram_word_frequency(sentences, 3)\n",
    "print(\"trigram probabilities\")\n",
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "44456788-9b55-4472-aed4-05145dce7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12- now we want to evaluate the n-gram language model, Perplexity is used to evaluate the model\n",
    "# performance on test set\n",
    "def perplexity_score(sentence, n_gram_counts, n_plus_one_gram_counts, vocab, k = 1.0):\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    # appending start and end token to the sentence\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    # convert sentence from list to tuple\n",
    "    sentence = tuple(sentence)\n",
    "    # length of sentence\n",
    "    N = len(sentence)\n",
    "    # initialize p_prod to be the product of n-gram probabilities\n",
    "    p_prod = 1.0\n",
    "    \n",
    "    # optional: \n",
    "    # log_probs = 0\n",
    "    \n",
    "    for t in range(n, N):\n",
    "        # getting prev n-gram\n",
    "        prev_n_gram = sentence[t - n : t]\n",
    "        # getting the current word\n",
    "        curr_word = sentence[t]\n",
    "        # probability of the curr word given n-gram\n",
    "        prob = build_n_gram_probability_with_k_smoothing(curr_word, prev_n_gram, n_gram_counts, n_plus_one_gram_counts, vocab, k)\n",
    "        # updating the p_prod\n",
    "        p_prod *= (1 / prob)\n",
    "        # updating log_probs\n",
    "        # log_probs += math.log(prob)\n",
    "        \n",
    "    # perplexity score is the N-th root of the comulative product of probability of the word given n-gram\n",
    "    perplexity_score = p_prod ** (1 / N)\n",
    "    # perplexity_score = log_probs * (1 / N)\n",
    "    # return perplexity score\n",
    "    return perplexity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "aadd0bc8-2000-4442-9493-9b4420cd23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "\n",
    "sentences_test = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "\n",
    "unique_words = list(set(sentences_test[0] + sentences_test[1]))\n",
    "\n",
    "unigram_counts = build_n_gram_word_frequency(sentences_test, 1)\n",
    "bigram_counts = build_n_gram_word_frequency(sentences_test, 2)\n",
    "\n",
    "perplexity_train1 = perplexity_score(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         unique_words, k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = perplexity_score(test_sentence,\n",
    "                                       unigram_counts, bigram_counts,\n",
    "                                       unique_words, k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3c8aa479-23eb-446d-a377-06e8923bfd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12- Building autocomplete model\n",
    "def suggest_word(previous_tokens, n_gram_counts, n_plus_one_gram_counts, vocab, k = 1.0, start_with = None):\n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    # get the words already inputted\n",
    "    prev_n_gram = previous_tokens[-n:]\n",
    "    # getting probability of the next words\n",
    "    probs = estimate_probabilities(prev_n_gram, n_gram_counts, n_plus_one_gram_counts, vocab, k)\n",
    "    # initilize suggested word to None\n",
    "    suggestion = None\n",
    "    # initialize max_prob of the suggested word to 0\n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probs.items():\n",
    "        if start_with is not None:\n",
    "            # execlude words that do not match with the start_with token\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        \n",
    "        # check if the current word is the most probable next word\n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b84f10f4-2820-40bb-b5cb-92357d67096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable word for the sentence\n",
      "['i', 'like']\n",
      "is\n",
      "(a) with probability 0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = build_n_gram_word_frequency(sentences, 1)\n",
    "bigram_counts = build_n_gram_word_frequency(sentences, 2)\n",
    "\n",
    "# inputted tokens\n",
    "input = [\"i\", \"like\"]\n",
    "\n",
    "suggestion, max_prob = suggest_word(input, unigram_counts, bigram_counts, unique_words, k = 1.0, start_with = None)\n",
    "print(f\"Most probable word for the sentence\\n{input}\\nis\\n({suggestion}) with probability {max_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d71d8e31-1b92-4060-92f6-8cbf3fa78625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13- define function to make multible suggestion based on n-gram\n",
    "def get_multible_suggestion(previous_tokens, n_gram_count_list, vocab, k = 1.0, start_with = None):\n",
    "    model_counts = len(n_gram_count_list)\n",
    "    # initialize variable to hold suggested words and their probbility\n",
    "    suggestions = []\n",
    "    probs = []\n",
    "    for i in range(model_counts - 1):\n",
    "        # get n_gram_counts\n",
    "        n_gram_counts = n_gram_count_list[i]\n",
    "        # get n_plus_one_gram_counts\n",
    "        n_plus_one_gram_counts = n_gram_count_list[i + 1]\n",
    "        # get suggested word\n",
    "        suggestion, prob = suggest_word(previous_tokens, n_gram_counts, n_plus_one_gram_counts, vocab, k = k, start_with = start_with)\n",
    "        # append suggesed word to suggestions\n",
    "        suggestions.append(suggestion)\n",
    "        probs.append(prob)\n",
    "    return suggestions, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4b5b18ff-e380-4eab-b4be-c05d3a6ee824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestions for '['i', 'like']'\n",
      "\n",
      "['a', 'a', 'a', 'a']\n",
      "[0.2727272727272727, 0.2, 0.1111111111111111, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = build_n_gram_word_frequency(sentences, 1)\n",
    "bigram_counts = build_n_gram_word_frequency(sentences, 2)\n",
    "trigram_counts = build_n_gram_word_frequency(sentences, 3)\n",
    "quadgram_counts = build_n_gram_word_frequency(sentences, 4)\n",
    "quintgram_counts = build_n_gram_word_frequency(sentences, 5)\n",
    "\n",
    "n_gram_count_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, quintgram_counts]\n",
    "input = [\"i\", \"like\"]\n",
    "\n",
    "suggestions, probs = get_multible_suggestion(input, n_gram_count_list, unique_words, k = 1.0, start_with = None)\n",
    "print(f\"Suggestions for '{input}'\")\n",
    "print()\n",
    "print(suggestions)\n",
    "print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
