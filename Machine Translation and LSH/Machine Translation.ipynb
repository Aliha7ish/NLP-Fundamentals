{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b19dfd-4254-4d38-86e4-6534868a4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "# import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "# from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# from utils import (cosine_similarity, get_dict,\n",
    "#                    process_tweet)\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bf8ae8-6e27-49c9-8f1e-aa79d354aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof\n",
    "\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    # you have to set this variable to the true label.\n",
    "    cos = -10\n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos\n",
    "\n",
    "# Procedure to plot and arrows that represents vectors with pyplot\n",
    "def plot_vectors(vectors, colors=['k', 'b', 'r', 'm', 'c'], axes=None, fname='image.svg', ax=None):\n",
    "    scale = 1\n",
    "    scale_units = 'x'\n",
    "    x_dir = []\n",
    "    y_dir = []\n",
    "    \n",
    "    for i, vec in enumerate(vectors):\n",
    "        x_dir.append(vec[0][0])\n",
    "        y_dir.append(vec[0][1])\n",
    "    \n",
    "    if ax == None:\n",
    "        fig, ax2 = plt.subplots()\n",
    "    else:\n",
    "        ax2 = ax\n",
    "      \n",
    "    if axes == None:\n",
    "        x_axis = 2 + np.max(np.abs(x_dir))\n",
    "        y_axis = 2 + np.max(np.abs(y_dir))\n",
    "    else:\n",
    "        x_axis = axes[0]\n",
    "        y_axis = axes[1]\n",
    "        \n",
    "    ax2.axis([-x_axis, x_axis, -y_axis, y_axis])\n",
    "        \n",
    "    for i, vec in enumerate(vectors):\n",
    "        ax2.arrow(0, 0, vec[0][0], vec[0][1], head_width=0.05 * x_axis, head_length=0.05 * y_axis, fc=colors[i], ec=colors[i])\n",
    "    \n",
    "    if ax == None:\n",
    "        plt.show()\n",
    "        fig.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2259712b-bb96-47fe-861a-dccbd80e3a66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[32m      3\u001b[39m en_embeddings = KeyedVectors.load_word2vec_format(\u001b[33m'\u001b[39m\u001b[33m./GoogleNews-vectors-negative300.bin\u001b[39m\u001b[33m'\u001b[39m, binary = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m fr_embeddings = KeyedVectors.load_word2vec_format(\u001b[33m'\u001b[39m\u001b[33m./wiki.multi.fr.vec\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'triu' from 'scipy.linalg' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n",
    "\n",
    "\n",
    "\n",
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the english to french test dictionary is', len(en_fr_train))\n",
    "\n",
    "english_set = set(en_embeddings.vocab)\n",
    "french_set = set(fr_embeddings.vocab)\n",
    "en_embeddings_subset = {}\n",
    "fr_embeddings_subset = {}\n",
    "french_words = set(en_fr_train.values())\n",
    "\n",
    "for en_word in en_fr_train.keys():\n",
    "    fr_word = en_fr_train[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "for en_word in en_fr_test.keys():\n",
    "    fr_word = en_fr_test[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "pickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\n",
    "pickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f90991-360d-4a25-961e-97fc585eb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_fr = {\n",
    "#     \"the\":  \"le\",\n",
    "# \"Ashgabat\": \"ashgabat\",\n",
    "# \"the\": \"les\",\n",
    "# \"the\": \"la\",\n",
    "# \"and\": \"et\",\n",
    "# \"was\": \"fut\",\n",
    "# \"was\": \"etait\",\n",
    "# \"was\": \"était\",\n",
    "# }\n",
    "\n",
    "# en_embeddings = {\n",
    "#     \"the\": [0.2, 0.3, 0.4, 0.5],\n",
    "#     \"Ashgabat\": [0.4444, 0.213, 0.14, 0.5],\n",
    "#     \"the\": [0.4444, 0.0000213, 0.00014, 0.5120],\n",
    "#     \"the\": [0.44004004, 0.000913, 0.01484, 0.145],\n",
    "#     \"and\": [0.42144, 0.28413, 0.70014, 0.0005],\n",
    "#     \"was\": [0.0004, 0.00023, 0.00007, 0.0006],\n",
    "#     \"was\": [0.0008, 0.00063, 0.00001, 0.0056],\n",
    "#     \"was\": [0.0098, 0.00763, 0.08001, 0.0756],\n",
    "# }\n",
    "\n",
    "# fr_embeddings = {\n",
    "#     \"le\": [0.27, 0.0013, 0.1204, 0.125],\n",
    "#     \"ashgabat\": [0.0044, 0.1213, 0.014, 0.05],\n",
    "#     \"les\": [0.0145, 0.0213, 0.0084, 0.20],\n",
    "#     \"la\": [0.004, 0.000913, 0.01484, 0.145],\n",
    "#     \"et\": [0.74, 0.13, 0.7, 0.05],\n",
    "#     \"fut\": [0.4, 0.23, 0.07, 0.06],\n",
    "#     \"etait\": [0.8, 0.63, 0.701, 0.0056],\n",
    "#     \"était\": [0.9, 0.763, 0.81, 0.756],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41cc269e-982b-4d4d-84b0-45badba81c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "\n",
    "def get_matrices(en_fr, en_embeddings, fr_embeddings):\n",
    "    X = np.empty((0, n_cols))\n",
    "    y = np.empty((0, n_cols))\n",
    "\n",
    "    en_emb_set = set(en_embeddings.keys())\n",
    "    fr_emb_set = set(fr_embeddings.keys())\n",
    "    \n",
    "    for en, fr in en_fr.items():\n",
    "        if en in en_emb_set and fr in fr_emb_set:\n",
    "            X_rows = np.array([en_embeddings[en]])\n",
    "            y_rows = np.array([fr_embeddings[fr]])\n",
    "            X = np.append(X, X_rows, axis = 0)\n",
    "            y = np.append(y, y_rows, axis = 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b437ee7e-00a7-42d8-b730-643e2062de30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.004   , 0.000913, 0.01484 , 0.145   ],\n",
       "       [0.0044  , 0.1213  , 0.014   , 0.05    ],\n",
       "       [0.74    , 0.13    , 0.7     , 0.05    ],\n",
       "       [0.9     , 0.763   , 0.81    , 0.756   ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = get_matrices(en_fr, en_embeddings, fr_embeddings)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f78ce80-c8e3-4b58-915a-70110154a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.dot(X, R)\n",
    "    A = y_pred - Y\n",
    "    f_a = np.linalg.norm(A, 'fro') ** 2\n",
    "    return f_a / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8bd0247-1a80-42ce-a420-1bc8313f6ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5876522214232947"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.random.rand(4, 4)\n",
    "compute_loss(X, y, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7d7ad53-e1b0-4c36-b301-ad0186e28195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    emd_pred = np.dot(X, R)\n",
    "    diff = emd_pred - Y\n",
    "    grad = (2 / m) * np.dot(X.T, diff)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10070d6e-1d77-464e-8659-7b11641ed80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2344706 ,  0.40319767,  0.21835551,  0.35856566],\n",
       "       [ 0.05409672,  0.19154266,  0.06034815,  0.19563832],\n",
       "       [-0.05005214,  0.34156598, -0.01701724,  0.32127527],\n",
       "       [ 0.21040403,  0.14660296,  0.18725503,  0.16921253]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(X, y, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddb98738-7c57-4a2d-ac54-dc3290444934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps = 100, lr = 0.0003):\n",
    "    np.random.seed(129)\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    \n",
    "    for step in range(train_steps):\n",
    "        if step % 25 == 0:\n",
    "            print(f\"Training Loss at EPOCH {step} ==> {compute_loss(X, Y, R)}\")\n",
    "            \n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        R -= lr * gradient\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6476ec1-d627-4418-a930-d341e96a12e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at EPOCH 0 ==> 3.7241601261682264\n",
      "Training Loss at EPOCH 25 ==> 3.628289198210136\n",
      "Training Loss at EPOCH 50 ==> 3.53497687566553\n",
      "Training Loss at EPOCH 75 ==> 3.4441545620485563\n",
      "Training Loss at EPOCH 100 ==> 3.3557555012458273\n",
      "Training Loss at EPOCH 125 ==> 3.2697147281352903\n",
      "Training Loss at EPOCH 150 ==> 3.185969020530152\n",
      "Training Loss at EPOCH 175 ==> 3.104456852412266\n",
      "Training Loss at EPOCH 200 ==> 3.025118348420393\n",
      "Training Loss at EPOCH 225 ==> 2.9478952395596627\n",
      "Training Loss at EPOCH 250 ==> 2.8727308200994743\n",
      "Training Loss at EPOCH 275 ==> 2.799569905627937\n",
      "Training Loss at EPOCH 300 ==> 2.7283587922318246\n",
      "Training Loss at EPOCH 325 ==> 2.659045216771834\n",
      "Training Loss at EPOCH 350 ==> 2.591578318223778\n",
      "Training Loss at EPOCH 375 ==> 2.5259086000570807\n",
      "Training Loss at EPOCH 400 ==> 2.461987893622769\n",
      "Training Loss at EPOCH 425 ==> 2.3997693225238583\n",
      "Training Loss at EPOCH 450 ==> 2.339207267941771\n",
      "Training Loss at EPOCH 475 ==> 2.280257334893146\n",
      "Training Loss at EPOCH 500 ==> 2.222876319392061\n",
      "Training Loss at EPOCH 525 ==> 2.1670221764933855\n",
      "Training Loss at EPOCH 550 ==> 2.1126539891936056\n",
      "Training Loss at EPOCH 575 ==> 2.0597319381661285\n",
      "Training Loss at EPOCH 600 ==> 2.0082172723086718\n",
      "Training Loss at EPOCH 625 ==> 1.9580722800809292\n",
      "Training Loss at EPOCH 650 ==> 1.909260261611338\n",
      "Training Loss at EPOCH 675 ==> 1.8617455015522872\n",
      "Training Loss at EPOCH 700 ==> 1.8154932426636823\n",
      "Training Loss at EPOCH 725 ==> 1.770469660105355\n",
      "Training Loss at EPOCH 750 ==> 1.726641836419251\n",
      "Training Loss at EPOCH 775 ==> 1.6839777371829228\n",
      "Training Loss at EPOCH 800 ==> 1.6424461873163025\n",
      "Training Loss at EPOCH 825 ==> 1.602016848024217\n",
      "Training Loss at EPOCH 850 ==> 1.562660194357605\n",
      "Training Loss at EPOCH 875 ==> 1.5243474933768184\n",
      "Training Loss at EPOCH 900 ==> 1.4870507829008583\n",
      "Training Loss at EPOCH 925 ==> 1.4507428508268263\n",
      "Training Loss at EPOCH 950 ==> 1.415397215004288\n",
      "Training Loss at EPOCH 975 ==> 1.380988103649653\n",
      "Training Loss at EPOCH 1000 ==> 1.3474904362860947\n",
      "Training Loss at EPOCH 1025 ==> 1.3148798051948984\n",
      "Training Loss at EPOCH 1050 ==> 1.28313245736451\n",
      "Training Loss at EPOCH 1075 ==> 1.252225276923947\n",
      "Training Loss at EPOCH 1100 ==> 1.2221357680475615\n",
      "Training Loss at EPOCH 1125 ==> 1.1928420383185068\n",
      "Training Loss at EPOCH 1150 ==> 1.1643227825386135\n",
      "Training Loss at EPOCH 1175 ==> 1.136557266972678\n",
      "Training Loss at EPOCH 1200 ==> 1.109525314015523\n",
      "Training Loss at EPOCH 1225 ==> 1.0832072872704728\n",
      "Training Loss at EPOCH 1250 ==> 1.0575840770282174\n",
      "Training Loss at EPOCH 1275 ==> 1.0326370861353058\n",
      "Training Loss at EPOCH 1300 ==> 1.0083482162418294\n",
      "Training Loss at EPOCH 1325 ==> 0.9846998544181143\n",
      "Training Loss at EPOCH 1350 ==> 0.9616748601305127\n",
      "Training Loss at EPOCH 1375 ==> 0.9392565525666754\n",
      "Training Loss at EPOCH 1400 ==> 0.9174286983009103\n",
      "Training Loss at EPOCH 1425 ==> 0.8961754992905163\n",
      "Training Loss at EPOCH 1450 ==> 0.8754815811941963\n",
      "Training Loss at EPOCH 1475 ==> 0.8553319820039247\n",
      "Training Loss at EPOCH 1500 ==> 0.8357121409818395\n",
      "Training Loss at EPOCH 1525 ==> 0.8166078878939939\n",
      "Training Loss at EPOCH 1550 ==> 0.7980054325329827\n",
      "Training Loss at EPOCH 1575 ==> 0.779891354521709\n",
      "Training Loss at EPOCH 1600 ==> 0.7622525933907355\n",
      "Training Loss at EPOCH 1625 ==> 0.745076438921884\n",
      "Training Loss at EPOCH 1650 ==> 0.7283505217509416\n",
      "Training Loss at EPOCH 1675 ==> 0.7120628042225163\n",
      "Training Loss at EPOCH 1700 ==> 0.6962015714902778\n",
      "Training Loss at EPOCH 1725 ==> 0.6807554228559979\n",
      "Training Loss at EPOCH 1750 ==> 0.6657132633409895\n",
      "Training Loss at EPOCH 1775 ==> 0.6510642954836992\n",
      "Training Loss at EPOCH 1800 ==> 0.6367980113573875\n",
      "Training Loss at EPOCH 1825 ==> 0.6229041848019974\n",
      "Training Loss at EPOCH 1850 ==> 0.6093728638644565\n",
      "Training Loss at EPOCH 1875 ==> 0.5961943634418234\n",
      "Training Loss at EPOCH 1900 ==> 0.5833592581218351\n",
      "Training Loss at EPOCH 1925 ==> 0.5708583752155574\n",
      "Training Loss at EPOCH 1950 ==> 0.558682787976986\n",
      "Training Loss at EPOCH 1975 ==> 0.5468238090045796\n",
      "Training Loss at EPOCH 2000 ==> 0.535272983819844\n",
      "Training Loss at EPOCH 2025 ==> 0.5240220846182183\n",
      "Training Loss at EPOCH 2050 ==> 0.5130631041876355\n",
      "Training Loss at EPOCH 2075 ==> 0.5023882499902632\n",
      "Training Loss at EPOCH 2100 ==> 0.49198993840304317\n",
      "Training Loss at EPOCH 2125 ==> 0.4818607891127701\n",
      "Training Loss at EPOCH 2150 ==> 0.471993619661559\n",
      "Training Loss at EPOCH 2175 ==> 0.4623814401386725\n",
      "Training Loss at EPOCH 2200 ==> 0.4530174480147762\n",
      "Training Loss at EPOCH 2225 ==> 0.44389502311480034\n",
      "Training Loss at EPOCH 2250 ==> 0.43500772272569127\n",
      "Training Loss at EPOCH 2275 ==> 0.4263492768354295\n",
      "Training Loss at EPOCH 2300 ==> 0.41791358349979807\n",
      "Training Loss at EPOCH 2325 ==> 0.4096947043334658\n",
      "Training Loss at EPOCH 2350 ==> 0.4016868601220547\n",
      "Training Loss at EPOCH 2375 ==> 0.39388442655194533\n",
      "Training Loss at EPOCH 2400 ==> 0.38628193005465816\n",
      "Training Loss at EPOCH 2425 ==> 0.37887404376273826\n",
      "Training Loss at EPOCH 2450 ==> 0.37165558357415174\n",
      "Training Loss at EPOCH 2475 ==> 0.36462150432228113\n",
      "Training Loss at EPOCH 2500 ==> 0.357766896048686\n",
      "Training Loss at EPOCH 2525 ==> 0.35108698037587394\n",
      "Training Loss at EPOCH 2550 ==> 0.34457710697739496\n",
      "Training Loss at EPOCH 2575 ==> 0.3382327501426499\n",
      "Training Loss at EPOCH 2600 ==> 0.33204950543387135\n",
      "Training Loss at EPOCH 2625 ==> 0.32602308643280253\n",
      "Training Loss at EPOCH 2650 ==> 0.3201493215746692\n",
      "Training Loss at EPOCH 2675 ==> 0.31442415106710037\n",
      "Training Loss at EPOCH 2700 ==> 0.3088436238917204\n",
      "Training Loss at EPOCH 2725 ==> 0.3034038948861927\n",
      "Training Loss at EPOCH 2750 ==> 0.2981012219045555\n",
      "Training Loss at EPOCH 2775 ==> 0.29293196305375274\n",
      "Training Loss at EPOCH 2800 ==> 0.287892574004311\n",
      "Training Loss at EPOCH 2825 ==> 0.2829796053731754\n",
      "Training Loss at EPOCH 2850 ==> 0.2781897001767683\n",
      "Training Loss at EPOCH 2875 ==> 0.27351959135238574\n",
      "Training Loss at EPOCH 2900 ==> 0.26896609934609805\n",
      "Training Loss at EPOCH 2925 ==> 0.26452612976537054\n",
      "Training Loss at EPOCH 2950 ==> 0.26019667109466704\n",
      "Training Loss at EPOCH 2975 ==> 0.255974792472349\n",
      "Training Loss at EPOCH 3000 ==> 0.25185764152721857\n",
      "Training Loss at EPOCH 3025 ==> 0.24784244227311497\n",
      "Training Loss at EPOCH 3050 ==> 0.24392649305999795\n",
      "Training Loss at EPOCH 3075 ==> 0.24010716458000708\n",
      "Training Loss at EPOCH 3100 ==> 0.23638189792702136\n",
      "Training Loss at EPOCH 3125 ==> 0.23274820270828153\n",
      "Training Loss at EPOCH 3150 ==> 0.22920365520668087\n",
      "Training Loss at EPOCH 3175 ==> 0.22574589659236216\n",
      "Training Loss at EPOCH 3200 ==> 0.22237263118230072\n",
      "Training Loss at EPOCH 3225 ==> 0.21908162474658335\n",
      "Training Loss at EPOCH 3250 ==> 0.21587070286013135\n",
      "Training Loss at EPOCH 3275 ==> 0.21273774929864916\n",
      "Training Loss at EPOCH 3300 ==> 0.2096807044776095\n",
      "Training Loss at EPOCH 3325 ==> 0.20669756393312216\n",
      "Training Loss at EPOCH 3350 ==> 0.2037863768435632\n",
      "Training Loss at EPOCH 3375 ==> 0.20094524459086832\n",
      "Training Loss at EPOCH 3400 ==> 0.1981723193604284\n",
      "Training Loss at EPOCH 3425 ==> 0.19546580277854997\n",
      "Training Loss at EPOCH 3450 ==> 0.1928239445864743\n",
      "Training Loss at EPOCH 3475 ==> 0.19024504134997225\n",
      "Training Loss at EPOCH 3500 ==> 0.18772743520356103\n",
      "Training Loss at EPOCH 3525 ==> 0.185269512628415\n",
      "Training Loss at EPOCH 3550 ==> 0.1828697032630647\n",
      "Training Loss at EPOCH 3575 ==> 0.18052647874600536\n",
      "Training Loss at EPOCH 3600 ==> 0.17823835158935822\n",
      "Training Loss at EPOCH 3625 ==> 0.17600387408275153\n",
      "Training Loss at EPOCH 3650 ==> 0.17382163722661106\n",
      "Training Loss at EPOCH 3675 ==> 0.17169026969406984\n",
      "Training Loss at EPOCH 3700 ==> 0.1696084368207297\n",
      "Training Loss at EPOCH 3725 ==> 0.1675748396215278\n",
      "Training Loss at EPOCH 3750 ==> 0.16558821383398004\n",
      "Training Loss at EPOCH 3775 ==> 0.1636473289870943\n",
      "Training Loss at EPOCH 3800 ==> 0.16175098749526395\n",
      "Training Loss at EPOCH 3825 ==> 0.15989802377647216\n",
      "Training Loss at EPOCH 3850 ==> 0.158087303394154\n",
      "Training Loss at EPOCH 3875 ==> 0.1563177222220827\n",
      "Training Loss at EPOCH 3900 ==> 0.15458820563166029\n",
      "Training Loss at EPOCH 3925 ==> 0.15289770770101319\n",
      "Training Loss at EPOCH 3950 ==> 0.1512452104453076\n",
      "Training Loss at EPOCH 3975 ==> 0.149629723067714\n",
      "Training Loss at EPOCH 4000 ==> 0.14805028123046807\n",
      "Training Loss at EPOCH 4025 ==> 0.14650594634548791\n",
      "Training Loss at EPOCH 4050 ==> 0.14499580488402358\n",
      "Training Loss at EPOCH 4075 ==> 0.14351896770482753\n",
      "Training Loss at EPOCH 4100 ==> 0.14207456940034957\n",
      "Training Loss at EPOCH 4125 ==> 0.14066176766047228\n",
      "Training Loss at EPOCH 4150 ==> 0.13927974265331683\n",
      "Training Loss at EPOCH 4175 ==> 0.1379276964226595\n",
      "Training Loss at EPOCH 4200 ==> 0.13660485230151703\n",
      "Training Loss at EPOCH 4225 ==> 0.13531045434146122\n",
      "Training Loss at EPOCH 4250 ==> 0.13404376675724675\n",
      "Training Loss at EPOCH 4275 ==> 0.1328040733863363\n",
      "Training Loss at EPOCH 4300 ==> 0.13159067716292727\n",
      "Training Loss at EPOCH 4325 ==> 0.13040289960608767\n",
      "Training Loss at EPOCH 4350 ==> 0.12924008032162507\n",
      "Training Loss at EPOCH 4375 ==> 0.1281015765173188\n",
      "Training Loss at EPOCH 4400 ==> 0.12698676253115704\n",
      "Training Loss at EPOCH 4425 ==> 0.12589502937223068\n",
      "Training Loss at EPOCH 4450 ==> 0.12482578427394309\n",
      "Training Loss at EPOCH 4475 ==> 0.12377845025920557\n",
      "Training Loss at EPOCH 4500 ==> 0.1227524657172984\n",
      "Training Loss at EPOCH 4525 ==> 0.12174728399208272\n",
      "Training Loss at EPOCH 4550 ==> 0.12076237298125861\n",
      "Training Loss at EPOCH 4575 ==> 0.11979721474637499\n",
      "Training Loss at EPOCH 4600 ==> 0.11885130513330053\n",
      "Training Loss at EPOCH 4625 ==> 0.11792415340287632\n",
      "Training Loss at EPOCH 4650 ==> 0.11701528187147639\n",
      "Training Loss at EPOCH 4675 ==> 0.1161242255612106\n",
      "Training Loss at EPOCH 4700 ==> 0.11525053185951177\n",
      "Training Loss at EPOCH 4725 ==> 0.11439376018785315\n",
      "Training Loss at EPOCH 4750 ==> 0.11355348167935378\n",
      "Training Loss at EPOCH 4775 ==> 0.11272927886503217\n",
      "Training Loss at EPOCH 4800 ==> 0.11192074536847543\n",
      "Training Loss at EPOCH 4825 ==> 0.11112748560869982\n",
      "Training Loss at EPOCH 4850 ==> 0.1103491145109812\n",
      "Training Loss at EPOCH 4875 ==> 0.10958525722544288\n",
      "Training Loss at EPOCH 4900 ==> 0.10883554885319169\n",
      "Training Loss at EPOCH 4925 ==> 0.10809963417980081\n",
      "Training Loss at EPOCH 4950 ==> 0.10737716741594097\n",
      "Training Loss at EPOCH 4975 ==> 0.10666781194496935\n",
      "Training Loss at EPOCH 5000 ==> 0.10597124007728906\n",
      "Training Loss at EPOCH 5025 ==> 0.10528713281129691\n",
      "Training Loss at EPOCH 5050 ==> 0.10461517960074425\n",
      "Training Loss at EPOCH 5075 ==> 0.10395507812833713\n",
      "Training Loss at EPOCH 5100 ==> 0.10330653408540949\n",
      "Training Loss at EPOCH 5125 ==> 0.10266926095750581\n",
      "Training Loss at EPOCH 5150 ==> 0.10204297981571497\n",
      "Training Loss at EPOCH 5175 ==> 0.10142741911360117\n",
      "Training Loss at EPOCH 5200 ==> 0.10082231448958148\n",
      "Training Loss at EPOCH 5225 ==> 0.10022740857460333\n",
      "Training Loss at EPOCH 5250 ==> 0.09964245080498121\n",
      "Training Loss at EPOCH 5275 ==> 0.09906719724025267\n",
      "Training Loss at EPOCH 5300 ==> 0.09850141038591975\n",
      "Training Loss at EPOCH 5325 ==> 0.09794485902094444\n",
      "Training Loss at EPOCH 5350 ==> 0.09739731802987092\n",
      "Training Loss at EPOCH 5375 ==> 0.09685856823945033\n",
      "Training Loss at EPOCH 5400 ==> 0.09632839625964673\n",
      "Training Loss at EPOCH 5425 ==> 0.09580659432890773\n",
      "Training Loss at EPOCH 5450 ==> 0.09529296016358428\n",
      "Training Loss at EPOCH 5475 ==> 0.09478729681138934\n",
      "Training Loss at EPOCH 5500 ==> 0.09428941250878618\n",
      "Training Loss at EPOCH 5525 ==> 0.09379912054220135\n",
      "Training Loss at EPOCH 5550 ==> 0.0933162391129595\n",
      "Training Loss at EPOCH 5575 ==> 0.0928405912058404\n",
      "Training Loss at EPOCH 5600 ==> 0.09237200446116106\n",
      "Training Loss at EPOCH 5625 ==> 0.09191031105028775\n",
      "Training Loss at EPOCH 5650 ==> 0.09145534755448645\n",
      "Training Loss at EPOCH 5675 ==> 0.09100695484702243\n",
      "Training Loss at EPOCH 5700 ==> 0.09056497797842074\n",
      "Training Loss at EPOCH 5725 ==> 0.09012926606480395\n",
      "Training Loss at EPOCH 5750 ==> 0.08969967217922388\n",
      "Training Loss at EPOCH 5775 ==> 0.08927605324590727\n",
      "Training Loss at EPOCH 5800 ==> 0.0888582699373371\n",
      "Training Loss at EPOCH 5825 ==> 0.08844618657409338\n",
      "Training Loss at EPOCH 5850 ==> 0.0880396710273798\n",
      "Training Loss at EPOCH 5875 ==> 0.08763859462416364\n",
      "Training Loss at EPOCH 5900 ==> 0.08724283205485923\n",
      "Training Loss at EPOCH 5925 ==> 0.08685226128348647\n",
      "Training Loss at EPOCH 5950 ==> 0.08646676346023818\n",
      "Training Loss at EPOCH 5975 ==> 0.08608622283639146\n",
      "Training Loss at EPOCH 6000 ==> 0.08571052668150056\n",
      "Training Loss at EPOCH 6025 ==> 0.08533956520280948\n",
      "Training Loss at EPOCH 6050 ==> 0.08497323146682512\n",
      "Training Loss at EPOCH 6075 ==> 0.08461142132299308\n",
      "Training Loss at EPOCH 6100 ==> 0.08425403332941955\n",
      "Training Loss at EPOCH 6125 ==> 0.08390096868058422\n",
      "Training Loss at EPOCH 6150 ==> 0.08355213113699113\n",
      "Training Loss at EPOCH 6175 ==> 0.08320742695670538\n",
      "Training Loss at EPOCH 6200 ==> 0.08286676482872495\n",
      "Training Loss at EPOCH 6225 ==> 0.08253005580813846\n",
      "Training Loss at EPOCH 6250 ==> 0.08219721325302119\n",
      "Training Loss at EPOCH 6275 ==> 0.0818681527630222\n",
      "Training Loss at EPOCH 6300 ==> 0.081542792119598\n",
      "Training Loss at EPOCH 6325 ==> 0.08122105122784758\n",
      "Training Loss at EPOCH 6350 ==> 0.0809028520599068\n",
      "Training Loss at EPOCH 6375 ==> 0.08058811859985948\n",
      "Training Loss at EPOCH 6400 ==> 0.08027677679012546\n",
      "Training Loss at EPOCH 6425 ==> 0.07996875447928477\n",
      "Training Loss at EPOCH 6450 ==> 0.07966398137130058\n",
      "Training Loss at EPOCH 6475 ==> 0.07936238897610262\n",
      "Training Loss at EPOCH 6500 ==> 0.07906391056149499\n",
      "Training Loss at EPOCH 6525 ==> 0.07876848110635261\n",
      "Training Loss at EPOCH 6550 ==> 0.07847603725507174\n",
      "Training Loss at EPOCH 6575 ==> 0.07818651727324102\n",
      "Training Loss at EPOCH 6600 ==> 0.07789986100450064\n",
      "Training Loss at EPOCH 6625 ==> 0.07761600982855663\n",
      "Training Loss at EPOCH 6650 ==> 0.07733490662032078\n",
      "Training Loss at EPOCH 6675 ==> 0.07705649571014496\n",
      "Training Loss at EPOCH 6700 ==> 0.0767807228451206\n",
      "Training Loss at EPOCH 6725 ==> 0.07650753515141542\n",
      "Training Loss at EPOCH 6750 ==> 0.07623688109761863\n",
      "Training Loss at EPOCH 6775 ==> 0.07596871045906822\n",
      "Training Loss at EPOCH 6800 ==> 0.07570297428313398\n",
      "Training Loss at EPOCH 6825 ==> 0.07543962485543008\n",
      "Training Loss at EPOCH 6850 ==> 0.0751786156669328\n",
      "Training Loss at EPOCH 6875 ==> 0.07491990138197918\n",
      "Training Loss at EPOCH 6900 ==> 0.07466343780712227\n",
      "Training Loss at EPOCH 6925 ==> 0.07440918186082089\n",
      "Training Loss at EPOCH 6950 ==> 0.0741570915439409\n",
      "Training Loss at EPOCH 6975 ==> 0.07390712591104633\n",
      "Training Loss at EPOCH 7000 ==> 0.07365924504245966\n",
      "Training Loss at EPOCH 7025 ==> 0.07341341001706969\n",
      "Training Loss at EPOCH 7050 ==> 0.07316958288586775\n",
      "Training Loss at EPOCH 7075 ==> 0.07292772664619265\n",
      "Training Loss at EPOCH 7100 ==> 0.07268780521666471\n",
      "Training Loss at EPOCH 7125 ==> 0.07244978341279115\n",
      "Training Loss at EPOCH 7150 ==> 0.07221362692322444\n",
      "Training Loss at EPOCH 7175 ==> 0.07197930228665618\n",
      "Training Loss at EPOCH 7200 ==> 0.07174677686932925\n",
      "Training Loss at EPOCH 7225 ==> 0.0715160188431524\n",
      "Training Loss at EPOCH 7250 ==> 0.07128699716439993\n",
      "Training Loss at EPOCH 7275 ==> 0.07105968155298173\n",
      "Training Loss at EPOCH 7300 ==> 0.07083404247226818\n",
      "Training Loss at EPOCH 7325 ==> 0.07061005110945462\n",
      "Training Loss at EPOCH 7350 ==> 0.07038767935645149\n",
      "Training Loss at EPOCH 7375 ==> 0.07016689979128556\n",
      "Training Loss at EPOCH 7400 ==> 0.06994768565999893\n",
      "Training Loss at EPOCH 7425 ==> 0.06973001085903213\n",
      "Training Loss at EPOCH 7450 ==> 0.06951384991807844\n",
      "Training Loss at EPOCH 7475 ==> 0.06929917798339716\n",
      "Training Loss at EPOCH 7500 ==> 0.06908597080157261\n",
      "Training Loss at EPOCH 7525 ==> 0.06887420470370818\n",
      "Training Loss at EPOCH 7550 ==> 0.06866385659004247\n",
      "Training Loss at EPOCH 7575 ==> 0.0684549039149771\n",
      "Training Loss at EPOCH 7600 ==> 0.068247324672505\n",
      "Training Loss at EPOCH 7625 ==> 0.06804109738202783\n",
      "Training Loss at EPOCH 7650 ==> 0.0678362010745531\n",
      "Training Loss at EPOCH 7675 ==> 0.06763261527925978\n",
      "Training Loss at EPOCH 7700 ==> 0.06743032001042332\n",
      "Training Loss at EPOCH 7725 ==> 0.06722929575469017\n",
      "Training Loss at EPOCH 7750 ==> 0.06702952345869206\n",
      "Training Loss at EPOCH 7775 ==> 0.06683098451699158\n",
      "Training Loss at EPOCH 7800 ==> 0.06663366076034975\n",
      "Training Loss at EPOCH 7825 ==> 0.066437534444307\n",
      "Training Loss at EPOCH 7850 ==> 0.06624258823806925\n",
      "Training Loss at EPOCH 7875 ==> 0.06604880521369086\n",
      "Training Loss at EPOCH 7900 ==> 0.06585616883554662\n",
      "Training Loss at EPOCH 7925 ==> 0.06566466295008469\n",
      "Training Loss at EPOCH 7950 ==> 0.06547427177585326\n",
      "Training Loss at EPOCH 7975 ==> 0.06528497989379371\n",
      "Training Loss at EPOCH 8000 ==> 0.06509677223779256\n",
      "Training Loss at EPOCH 8025 ==> 0.06490963408548596\n",
      "Training Loss at EPOCH 8050 ==> 0.06472355104930952\n",
      "Training Loss at EPOCH 8075 ==> 0.06453850906778683\n",
      "Training Loss at EPOCH 8100 ==> 0.06435449439705078\n",
      "Training Loss at EPOCH 8125 ==> 0.06417149360259093\n",
      "Training Loss at EPOCH 8150 ==> 0.06398949355122094\n",
      "Training Loss at EPOCH 8175 ==> 0.06380848140326059\n",
      "Training Loss at EPOCH 8200 ==> 0.06362844460492599\n",
      "Training Loss at EPOCH 8225 ==> 0.06344937088092292\n",
      "Training Loss at EPOCH 8250 ==> 0.06327124822723755\n",
      "Training Loss at EPOCH 8275 ==> 0.06309406490411966\n",
      "Training Loss at EPOCH 8300 ==> 0.06291780942925243\n",
      "Training Loss at EPOCH 8325 ==> 0.06274247057110456\n",
      "Training Loss at EPOCH 8350 ==> 0.06256803734245985\n",
      "Training Loss at EPOCH 8375 ==> 0.06239449899411832\n",
      "Training Loss at EPOCH 8400 ==> 0.06222184500876617\n",
      "Training Loss at EPOCH 8425 ==> 0.06205006509500819\n",
      "Training Loss at EPOCH 8450 ==> 0.06187914918155955\n",
      "Training Loss at EPOCH 8475 ==> 0.06170908741159229\n",
      "Training Loss at EPOCH 8500 ==> 0.061539870137231924\n",
      "Training Loss at EPOCH 8525 ==> 0.06137148791420083\n",
      "Training Loss at EPOCH 8550 ==> 0.061203931496604316\n",
      "Training Loss at EPOCH 8575 ==> 0.061037191831855055\n",
      "Training Loss at EPOCH 8600 ==> 0.06087126005573292\n",
      "Training Loss at EPOCH 8625 ==> 0.06070612748757581\n",
      "Training Loss at EPOCH 8650 ==> 0.06054178562559892\n",
      "Training Loss at EPOCH 8675 ==> 0.06037822614233784\n",
      "Training Loss at EPOCH 8700 ==> 0.060215440880213314\n",
      "Training Loss at EPOCH 8725 ==> 0.06005342184721348\n",
      "Training Loss at EPOCH 8750 ==> 0.05989216121269118\n",
      "Training Loss at EPOCH 8775 ==> 0.05973165130327265\n",
      "Training Loss at EPOCH 8800 ==> 0.05957188459887488\n",
      "Training Loss at EPOCH 8825 ==> 0.05941285372882902\n",
      "Training Loss at EPOCH 8850 ==> 0.059254551468106334\n",
      "Training Loss at EPOCH 8875 ==> 0.05909697073364446\n",
      "Training Loss at EPOCH 8900 ==> 0.0589401045807713\n",
      "Training Loss at EPOCH 8925 ==> 0.058783946199723526\n",
      "Training Loss at EPOCH 8950 ==> 0.058628488912257484\n",
      "Training Loss at EPOCH 8975 ==> 0.058473726168350226\n",
      "Training Loss at EPOCH 9000 ==> 0.05831965154298745\n",
      "Training Loss at EPOCH 9025 ==> 0.05816625873303706\n",
      "Training Loss at EPOCH 9050 ==> 0.05801354155420535\n",
      "Training Loss at EPOCH 9075 ==> 0.057861493938073716\n",
      "Training Loss at EPOCH 9100 ==> 0.05771010992921398\n",
      "Training Loss at EPOCH 9125 ==> 0.057559383682379985\n",
      "Training Loss at EPOCH 9150 ==> 0.057409309459773517\n",
      "Training Loss at EPOCH 9175 ==> 0.05725988162838251\n",
      "Training Loss at EPOCH 9200 ==> 0.05711109465738976\n",
      "Training Loss at EPOCH 9225 ==> 0.05696294311565001\n",
      "Training Loss at EPOCH 9250 ==> 0.0568154216692337\n",
      "Training Loss at EPOCH 9275 ==> 0.05666852507903571\n",
      "Training Loss at EPOCH 9300 ==> 0.056522248198447235\n",
      "Training Loss at EPOCH 9325 ==> 0.056376585971089\n",
      "Training Loss at EPOCH 9350 ==> 0.056231533428604354\n",
      "Training Loss at EPOCH 9375 ==> 0.05608708568851071\n",
      "Training Loss at EPOCH 9400 ==> 0.055943237952107414\n",
      "Training Loss at EPOCH 9425 ==> 0.055799985502438934\n",
      "Training Loss at EPOCH 9450 ==> 0.055657323702311655\n",
      "Training Loss at EPOCH 9475 ==> 0.05551524799236283\n",
      "Training Loss at EPOCH 9500 ==> 0.05537375388918051\n",
      "Training Loss at EPOCH 9525 ==> 0.05523283698347285\n",
      "Training Loss at EPOCH 9550 ==> 0.055092492938285643\n",
      "Training Loss at EPOCH 9575 ==> 0.054952717487266566\n",
      "Training Loss at EPOCH 9600 ==> 0.05481350643297528\n",
      "Training Loss at EPOCH 9625 ==> 0.05467485564523774\n",
      "Training Loss at EPOCH 9650 ==> 0.05453676105954369\n",
      "Training Loss at EPOCH 9675 ==> 0.05439921867548636\n",
      "Training Loss at EPOCH 9700 ==> 0.054262224555242974\n",
      "Training Loss at EPOCH 9725 ==> 0.05412577482209524\n",
      "Training Loss at EPOCH 9750 ==> 0.05398986565898831\n",
      "Training Loss at EPOCH 9775 ==> 0.053854493307127806\n",
      "Training Loss at EPOCH 9800 ==> 0.0537196540646134\n",
      "Training Loss at EPOCH 9825 ==> 0.05358534428510817\n",
      "Training Loss at EPOCH 9850 ==> 0.053451560376542716\n",
      "Training Loss at EPOCH 9875 ==> 0.053318298799853156\n",
      "Training Loss at EPOCH 9900 ==> 0.05318555606775225\n",
      "Training Loss at EPOCH 9925 ==> 0.05305332874353219\n",
      "Training Loss at EPOCH 9950 ==> 0.052921613439899075\n",
      "Training Loss at EPOCH 9975 ==> 0.05279040681783742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.15470112, -0.12114165,  0.17425389, -0.05234771,  0.36958048],\n",
       "       [-0.00751705,  0.20129935, -0.07709541, -0.10961917, -0.1403363 ],\n",
       "       [ 0.23015578, -0.40786196, -0.0575964 ,  0.0784273 , -0.06366766],\n",
       "       [-0.14405517,  0.36661322,  0.08600006,  0.44312352,  0.14785688],\n",
       "       [-0.14722985,  0.15661418,  0.04303335, -0.16227744, -0.07008161]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X_test = np.random.rand(m, n)\n",
    "Y_test = np.random.rand(m, n) * 0.1\n",
    "\n",
    "R = align_embeddings(X_test, Y_test, train_steps = 10000, lr = 0.0003)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfd44679-6ec5-4062-b7c5-46b64e5eb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at EPOCH 0 ==> 0.8938798305862191\n",
      "Training Loss at EPOCH 25 ==> 0.8922999292030824\n",
      "Training Loss at EPOCH 50 ==> 0.8907315077416809\n",
      "Training Loss at EPOCH 75 ==> 0.8891744740819633\n",
      "Training Loss at EPOCH 100 ==> 0.8876287368636618\n",
      "Training Loss at EPOCH 125 ==> 0.8860942054799766\n",
      "Training Loss at EPOCH 150 ==> 0.8845707900713128\n",
      "Training Loss at EPOCH 175 ==> 0.8830584015190684\n",
      "Training Loss at EPOCH 200 ==> 0.8815569514394757\n",
      "Training Loss at EPOCH 225 ==> 0.8800663521774932\n",
      "Training Loss at EPOCH 250 ==> 0.8785865168007492\n",
      "Training Loss at EPOCH 275 ==> 0.8771173590935331\n",
      "Training Loss at EPOCH 300 ==> 0.8756587935508412\n",
      "Training Loss at EPOCH 325 ==> 0.874210735372468\n",
      "Training Loss at EPOCH 350 ==> 0.8727731004571496\n",
      "Training Loss at EPOCH 375 ==> 0.8713458053967533\n",
      "Training Loss at EPOCH 400 ==> 0.8699287674705188\n",
      "Training Loss at EPOCH 425 ==> 0.8685219046393445\n",
      "Training Loss at EPOCH 450 ==> 0.8671251355401218\n",
      "Training Loss at EPOCH 475 ==> 0.865738379480119\n",
      "Training Loss at EPOCH 500 ==> 0.864361556431409\n",
      "Training Loss at EPOCH 525 ==> 0.8629945870253458\n",
      "Training Loss at EPOCH 550 ==> 0.861637392547084\n",
      "Training Loss at EPOCH 575 ==> 0.8602898949301483\n",
      "Training Loss at EPOCH 600 ==> 0.858952016751044\n",
      "Training Loss at EPOCH 625 ==> 0.8576236812239144\n",
      "Training Loss at EPOCH 650 ==> 0.8563048121952437\n",
      "Training Loss at EPOCH 675 ==> 0.8549953341386016\n",
      "Training Loss at EPOCH 700 ==> 0.8536951721494327\n",
      "Training Loss at EPOCH 725 ==> 0.8524042519398913\n",
      "Training Loss at EPOCH 750 ==> 0.8511224998337152\n",
      "Training Loss at EPOCH 775 ==> 0.8498498427611456\n",
      "Training Loss at EPOCH 800 ==> 0.8485862082538888\n",
      "Training Loss at EPOCH 825 ==> 0.8473315244401174\n",
      "Training Loss at EPOCH 850 ==> 0.8460857200395161\n",
      "Training Loss at EPOCH 875 ==> 0.8448487243583683\n",
      "Training Loss at EPOCH 900 ==> 0.8436204672846819\n",
      "Training Loss at EPOCH 925 ==> 0.8424008792833557\n",
      "Training Loss at EPOCH 950 ==> 0.8411898913913902\n",
      "Training Loss at EPOCH 975 ==> 0.8399874352131326\n",
      "Training Loss at EPOCH 1000 ==> 0.8387934429155642\n",
      "Training Loss at EPOCH 1025 ==> 0.8376078472236288\n",
      "Training Loss at EPOCH 1050 ==> 0.8364305814155959\n",
      "Training Loss at EPOCH 1075 ==> 0.8352615793184651\n",
      "Training Loss at EPOCH 1100 ==> 0.8341007753034103\n",
      "Training Loss at EPOCH 1125 ==> 0.832948104281257\n",
      "Training Loss at EPOCH 1150 ==> 0.8318035016980019\n",
      "Training Loss at EPOCH 1175 ==> 0.8306669035303686\n",
      "Training Loss at EPOCH 1200 ==> 0.829538246281398\n",
      "Training Loss at EPOCH 1225 ==> 0.8284174669760794\n",
      "Training Loss at EPOCH 1250 ==> 0.8273045031570136\n",
      "Training Loss at EPOCH 1275 ==> 0.8261992928801161\n",
      "Training Loss at EPOCH 1300 ==> 0.825101774710354\n",
      "Training Loss at EPOCH 1325 ==> 0.8240118877175172\n",
      "Training Loss at EPOCH 1350 ==> 0.8229295714720298\n",
      "Training Loss at EPOCH 1375 ==> 0.8218547660407892\n",
      "Training Loss at EPOCH 1400 ==> 0.8207874119830456\n",
      "Training Loss at EPOCH 1425 ==> 0.8197274503463132\n",
      "Training Loss at EPOCH 1450 ==> 0.8186748226623172\n",
      "Training Loss at EPOCH 1475 ==> 0.8176294709429716\n",
      "Training Loss at EPOCH 1500 ==> 0.8165913376763937\n",
      "Training Loss at EPOCH 1525 ==> 0.8155603658229506\n",
      "Training Loss at EPOCH 1550 ==> 0.8145364988113366\n",
      "Training Loss at EPOCH 1575 ==> 0.8135196805346881\n",
      "Training Loss at EPOCH 1600 ==> 0.8125098553467247\n",
      "Training Loss at EPOCH 1625 ==> 0.8115069680579281\n",
      "Training Loss at EPOCH 1650 ==> 0.8105109639317488\n",
      "Training Loss at EPOCH 1675 ==> 0.8095217886808463\n",
      "Training Loss at EPOCH 1700 ==> 0.8085393884633614\n",
      "Training Loss at EPOCH 1725 ==> 0.8075637098792162\n",
      "Training Loss at EPOCH 1750 ==> 0.8065946999664487\n",
      "Training Loss at EPOCH 1775 ==> 0.8056323061975768\n",
      "Training Loss at EPOCH 1800 ==> 0.8046764764759899\n",
      "Training Loss at EPOCH 1825 ==> 0.8037271591323744\n",
      "Training Loss at EPOCH 1850 ==> 0.8027843029211676\n",
      "Training Loss at EPOCH 1875 ==> 0.8018478570170392\n",
      "Training Loss at EPOCH 1900 ==> 0.8009177710114053\n",
      "Training Loss at EPOCH 1925 ==> 0.7999939949089696\n",
      "Training Loss at EPOCH 1950 ==> 0.7990764791242926\n",
      "Training Loss at EPOCH 1975 ==> 0.7981651744783917\n",
      "Training Loss at EPOCH 2000 ==> 0.7972600321953682\n",
      "Training Loss at EPOCH 2025 ==> 0.7963610038990613\n",
      "Training Loss at EPOCH 2050 ==> 0.7954680416097328\n",
      "Training Loss at EPOCH 2075 ==> 0.7945810977407769\n",
      "Training Loss at EPOCH 2100 ==> 0.7937001250954586\n",
      "Training Loss at EPOCH 2125 ==> 0.7928250768636786\n",
      "Training Loss at EPOCH 2150 ==> 0.7919559066187665\n",
      "Training Loss at EPOCH 2175 ==> 0.7910925683142978\n",
      "Training Loss at EPOCH 2200 ==> 0.7902350162809415\n",
      "Training Loss at EPOCH 2225 ==> 0.7893832052233303\n",
      "Training Loss at EPOCH 2250 ==> 0.7885370902169588\n",
      "Training Loss at EPOCH 2275 ==> 0.7876966267051069\n",
      "Training Loss at EPOCH 2300 ==> 0.7868617704957891\n",
      "Training Loss at EPOCH 2325 ==> 0.7860324777587292\n",
      "Training Loss at EPOCH 2350 ==> 0.7852087050223595\n",
      "Training Loss at EPOCH 2375 ==> 0.7843904091708461\n",
      "Training Loss at EPOCH 2400 ==> 0.7835775474411381\n",
      "Training Loss at EPOCH 2425 ==> 0.7827700774200418\n",
      "Training Loss at EPOCH 2450 ==> 0.7819679570413186\n",
      "Training Loss at EPOCH 2475 ==> 0.7811711445828082\n",
      "Training Loss at EPOCH 2500 ==> 0.780379598663576\n",
      "Training Loss at EPOCH 2525 ==> 0.7795932782410806\n",
      "Training Loss at EPOCH 2550 ==> 0.7788121426083707\n",
      "Training Loss at EPOCH 2575 ==> 0.7780361513913004\n",
      "Training Loss at EPOCH 2600 ==> 0.7772652645457705\n",
      "Training Loss at EPOCH 2625 ==> 0.7764994423549917\n",
      "Training Loss at EPOCH 2650 ==> 0.7757386454267704\n",
      "Training Loss at EPOCH 2675 ==> 0.7749828346908174\n",
      "Training Loss at EPOCH 2700 ==> 0.7742319713960795\n",
      "Training Loss at EPOCH 2725 ==> 0.7734860171080931\n",
      "Training Loss at EPOCH 2750 ==> 0.7727449337063581\n",
      "Training Loss at EPOCH 2775 ==> 0.772008683381737\n",
      "Training Loss at EPOCH 2800 ==> 0.7712772286338726\n",
      "Training Loss at EPOCH 2825 ==> 0.7705505322686289\n",
      "Training Loss at EPOCH 2850 ==> 0.7698285573955528\n",
      "Training Loss at EPOCH 2875 ==> 0.7691112674253561\n",
      "Training Loss at EPOCH 2900 ==> 0.7683986260674212\n",
      "Training Loss at EPOCH 2925 ==> 0.7676905973273231\n",
      "Training Loss at EPOCH 2950 ==> 0.7669871455043765\n",
      "Training Loss at EPOCH 2975 ==> 0.7662882351892012\n",
      "Training Loss at EPOCH 3000 ==> 0.7655938312613066\n",
      "Training Loss at EPOCH 3025 ==> 0.7649038988866997\n",
      "Training Loss at EPOCH 3050 ==> 0.7642184035155088\n",
      "Training Loss at EPOCH 3075 ==> 0.7635373108796313\n",
      "Training Loss at EPOCH 3100 ==> 0.7628605869903976\n",
      "Training Loss at EPOCH 3125 ==> 0.7621881981362556\n",
      "Training Loss at EPOCH 3150 ==> 0.761520110880476\n",
      "Training Loss at EPOCH 3175 ==> 0.7608562920588733\n",
      "Training Loss at EPOCH 3200 ==> 0.7601967087775503\n",
      "Training Loss at EPOCH 3225 ==> 0.7595413284106568\n",
      "Training Loss at EPOCH 3250 ==> 0.7588901185981703\n",
      "Training Loss at EPOCH 3275 ==> 0.7582430472436932\n",
      "Training Loss at EPOCH 3300 ==> 0.7576000825122697\n",
      "Training Loss at EPOCH 3325 ==> 0.7569611928282194\n",
      "Training Loss at EPOCH 3350 ==> 0.7563263468729908\n",
      "Training Loss at EPOCH 3375 ==> 0.7556955135830303\n",
      "Training Loss at EPOCH 3400 ==> 0.7550686621476727\n",
      "Training Loss at EPOCH 3425 ==> 0.7544457620070425\n",
      "Training Loss at EPOCH 3450 ==> 0.753826782849982\n",
      "Training Loss at EPOCH 3475 ==> 0.7532116946119879\n",
      "Training Loss at EPOCH 3500 ==> 0.7526004674731707\n",
      "Training Loss at EPOCH 3525 ==> 0.7519930718562284\n",
      "Training Loss at EPOCH 3550 ==> 0.7513894784244382\n",
      "Training Loss at EPOCH 3575 ==> 0.750789658079664\n",
      "Training Loss at EPOCH 3600 ==> 0.7501935819603819\n",
      "Training Loss at EPOCH 3625 ==> 0.7496012214397206\n",
      "Training Loss at EPOCH 3650 ==> 0.7490125481235189\n",
      "Training Loss at EPOCH 3675 ==> 0.7484275338483998\n",
      "Training Loss at EPOCH 3700 ==> 0.7478461506798586\n",
      "Training Loss at EPOCH 3725 ==> 0.74726837091037\n",
      "Training Loss at EPOCH 3750 ==> 0.7466941670575088\n",
      "Training Loss at EPOCH 3775 ==> 0.7461235118620861\n",
      "Training Loss at EPOCH 3800 ==> 0.7455563782863028\n",
      "Training Loss at EPOCH 3825 ==> 0.7449927395119159\n",
      "Training Loss at EPOCH 3850 ==> 0.7444325689384234\n",
      "Training Loss at EPOCH 3875 ==> 0.7438758401812602\n",
      "Training Loss at EPOCH 3900 ==> 0.7433225270700129\n",
      "Training Loss at EPOCH 3925 ==> 0.7427726036466473\n",
      "Training Loss at EPOCH 3950 ==> 0.7422260441637509\n",
      "Training Loss at EPOCH 3975 ==> 0.7416828230827899\n",
      "Training Loss at EPOCH 4000 ==> 0.7411429150723814\n",
      "Training Loss at EPOCH 4025 ==> 0.7406062950065806\n",
      "Training Loss at EPOCH 4050 ==> 0.7400729379631796\n",
      "Training Loss at EPOCH 4075 ==> 0.7395428192220226\n",
      "Training Loss at EPOCH 4100 ==> 0.7390159142633349\n",
      "Training Loss at EPOCH 4125 ==> 0.7384921987660653\n",
      "Training Loss at EPOCH 4150 ==> 0.7379716486062416\n",
      "Training Loss at EPOCH 4175 ==> 0.7374542398553425\n",
      "Training Loss at EPOCH 4200 ==> 0.7369399487786791\n",
      "Training Loss at EPOCH 4225 ==> 0.7364287518337923\n",
      "Training Loss at EPOCH 4250 ==> 0.7359206256688651\n",
      "Training Loss at EPOCH 4275 ==> 0.7354155471211431\n",
      "Training Loss at EPOCH 4300 ==> 0.7349134932153738\n",
      "Training Loss at EPOCH 4325 ==> 0.7344144411622548\n",
      "Training Loss at EPOCH 4350 ==> 0.7339183683568963\n",
      "Training Loss at EPOCH 4375 ==> 0.7334252523772982\n",
      "Training Loss at EPOCH 4400 ==> 0.7329350709828354\n",
      "Training Loss at EPOCH 4425 ==> 0.7324478021127613\n",
      "Training Loss at EPOCH 4450 ==> 0.7319634238847185\n",
      "Training Loss at EPOCH 4475 ==> 0.7314819145932663\n",
      "Training Loss at EPOCH 4500 ==> 0.7310032527084162\n",
      "Training Loss at EPOCH 4525 ==> 0.7305274168741842\n",
      "Training Loss at EPOCH 4550 ==> 0.7300543859071507\n",
      "Training Loss at EPOCH 4575 ==> 0.7295841387950358\n",
      "Training Loss at EPOCH 4600 ==> 0.7291166546952844\n",
      "Training Loss at EPOCH 4625 ==> 0.7286519129336638\n",
      "Training Loss at EPOCH 4650 ==> 0.7281898930028735\n",
      "Training Loss at EPOCH 4675 ==> 0.7277305745611647\n",
      "Training Loss at EPOCH 4700 ==> 0.7272739374309741\n",
      "Training Loss at EPOCH 4725 ==> 0.7268199615975662\n",
      "Training Loss at EPOCH 4750 ==> 0.7263686272076886\n",
      "Training Loss at EPOCH 4775 ==> 0.7259199145682392\n",
      "Training Loss at EPOCH 4800 ==> 0.7254738041449414\n",
      "Training Loss at EPOCH 4825 ==> 0.7250302765610337\n",
      "Training Loss at EPOCH 4850 ==> 0.7245893125959679\n",
      "Training Loss at EPOCH 4875 ==> 0.7241508931841206\n",
      "Training Loss at EPOCH 4900 ==> 0.7237149994135111\n",
      "Training Loss at EPOCH 4925 ==> 0.7232816125245359\n",
      "Training Loss at EPOCH 4950 ==> 0.7228507139087076\n",
      "Training Loss at EPOCH 4975 ==> 0.7224222851074092\n",
      "Training Loss at EPOCH 5000 ==> 0.7219963078106557\n",
      "Training Loss at EPOCH 5025 ==> 0.7215727638558678\n",
      "Training Loss at EPOCH 5050 ==> 0.7211516352266539\n",
      "Training Loss at EPOCH 5075 ==> 0.7207329040516052\n",
      "Training Loss at EPOCH 5100 ==> 0.7203165526030966\n",
      "Training Loss at EPOCH 5125 ==> 0.7199025632961025\n",
      "Training Loss at EPOCH 5150 ==> 0.7194909186870174\n",
      "Training Loss at EPOCH 5175 ==> 0.7190816014724916\n",
      "Training Loss at EPOCH 5200 ==> 0.7186745944882709\n",
      "Training Loss at EPOCH 5225 ==> 0.7182698807080509\n",
      "Training Loss at EPOCH 5250 ==> 0.7178674432423381\n",
      "Training Loss at EPOCH 5275 ==> 0.7174672653373205\n",
      "Training Loss at EPOCH 5300 ==> 0.7170693303737503\n",
      "Training Loss at EPOCH 5325 ==> 0.7166736218658304\n",
      "Training Loss at EPOCH 5350 ==> 0.7162801234601173\n",
      "Training Loss at EPOCH 5375 ==> 0.7158888189344265\n",
      "Training Loss at EPOCH 5400 ==> 0.7154996921967507\n",
      "Training Loss at EPOCH 5425 ==> 0.7151127272841883\n",
      "Training Loss at EPOCH 5450 ==> 0.7147279083618747\n",
      "Training Loss at EPOCH 5475 ==> 0.7143452197219304\n",
      "Training Loss at EPOCH 5500 ==> 0.713964645782411\n",
      "Training Loss at EPOCH 5525 ==> 0.7135861710862704\n",
      "Training Loss at EPOCH 5550 ==> 0.7132097803003311\n",
      "Training Loss at EPOCH 5575 ==> 0.7128354582142613\n",
      "Training Loss at EPOCH 5600 ==> 0.712463189739565\n",
      "Training Loss at EPOCH 5625 ==> 0.7120929599085755\n",
      "Training Loss at EPOCH 5650 ==> 0.7117247538734601\n",
      "Training Loss at EPOCH 5675 ==> 0.7113585569052328\n",
      "Training Loss at EPOCH 5700 ==> 0.7109943543927744\n",
      "Training Loss at EPOCH 5725 ==> 0.7106321318418617\n",
      "Training Loss at EPOCH 5750 ==> 0.7102718748742037\n",
      "Training Loss at EPOCH 5775 ==> 0.7099135692264869\n",
      "Training Loss at EPOCH 5800 ==> 0.7095572007494275\n",
      "Training Loss at EPOCH 5825 ==> 0.7092027554068325\n",
      "Training Loss at EPOCH 5850 ==> 0.7088502192746678\n",
      "Training Loss at EPOCH 5875 ==> 0.7084995785401342\n",
      "Training Loss at EPOCH 5900 ==> 0.7081508195007519\n",
      "Training Loss at EPOCH 5925 ==> 0.7078039285634505\n",
      "Training Loss at EPOCH 5950 ==> 0.7074588922436703\n",
      "Training Loss at EPOCH 5975 ==> 0.7071156971644654\n",
      "Training Loss at EPOCH 6000 ==> 0.7067743300556218\n",
      "Training Loss at EPOCH 6025 ==> 0.7064347777527747\n",
      "Training Loss at EPOCH 6050 ==> 0.7060970271965396\n",
      "Training Loss at EPOCH 6075 ==> 0.7057610654316465\n",
      "Training Loss at EPOCH 6100 ==> 0.7054268796060847\n",
      "Training Loss at EPOCH 6125 ==> 0.705094456970251\n",
      "Training Loss at EPOCH 6150 ==> 0.7047637848761072\n",
      "Training Loss at EPOCH 6175 ==> 0.7044348507763463\n",
      "Training Loss at EPOCH 6200 ==> 0.7041076422235604\n",
      "Training Loss at EPOCH 6225 ==> 0.703782146869421\n",
      "Training Loss at EPOCH 6250 ==> 0.7034583524638632\n",
      "Training Loss at EPOCH 6275 ==> 0.7031362468542767\n",
      "Training Loss at EPOCH 6300 ==> 0.7028158179847048\n",
      "Training Loss at EPOCH 6325 ==> 0.7024970538950495\n",
      "Training Loss at EPOCH 6350 ==> 0.7021799427202821\n",
      "Training Loss at EPOCH 6375 ==> 0.7018644726896623\n",
      "Training Loss at EPOCH 6400 ==> 0.7015506321259624\n",
      "Training Loss at EPOCH 6425 ==> 0.7012384094446975\n",
      "Training Loss at EPOCH 6450 ==> 0.7009277931533655\n",
      "Training Loss at EPOCH 6475 ==> 0.7006187718506877\n",
      "Training Loss at EPOCH 6500 ==> 0.7003113342258608\n",
      "Training Loss at EPOCH 6525 ==> 0.7000054690578129\n",
      "Training Loss at EPOCH 6550 ==> 0.699701165214465\n",
      "Training Loss at EPOCH 6575 ==> 0.6993984116520011\n",
      "Training Loss at EPOCH 6600 ==> 0.6990971974141418\n",
      "Training Loss at EPOCH 6625 ==> 0.6987975116314249\n",
      "Training Loss at EPOCH 6650 ==> 0.6984993435204927\n",
      "Training Loss at EPOCH 6675 ==> 0.6982026823833839\n",
      "Training Loss at EPOCH 6700 ==> 0.6979075176068332\n",
      "Training Loss at EPOCH 6725 ==> 0.6976138386615741\n",
      "Training Loss at EPOCH 6750 ==> 0.6973216351016491\n",
      "Training Loss at EPOCH 6775 ==> 0.6970308965637272\n",
      "Training Loss at EPOCH 6800 ==> 0.6967416127664234\n",
      "Training Loss at EPOCH 6825 ==> 0.6964537735096266\n",
      "Training Loss at EPOCH 6850 ==> 0.6961673686738321\n",
      "Training Loss at EPOCH 6875 ==> 0.69588238821948\n",
      "Training Loss at EPOCH 6900 ==> 0.695598822186299\n",
      "Training Loss at EPOCH 6925 ==> 0.6953166606926554\n",
      "Training Loss at EPOCH 6950 ==> 0.6950358939349075\n",
      "Training Loss at EPOCH 6975 ==> 0.6947565121867656\n",
      "Training Loss at EPOCH 7000 ==> 0.6944785057986573\n",
      "Training Loss at EPOCH 7025 ==> 0.6942018651970979\n",
      "Training Loss at EPOCH 7050 ==> 0.6939265808840668\n",
      "Training Loss at EPOCH 7075 ==> 0.6936526434363874\n",
      "Training Loss at EPOCH 7100 ==> 0.6933800435051136\n",
      "Training Loss at EPOCH 7125 ==> 0.6931087718149206\n",
      "Training Loss at EPOCH 7150 ==> 0.6928388191635025\n",
      "Training Loss at EPOCH 7175 ==> 0.6925701764209712\n",
      "Training Loss at EPOCH 7200 ==> 0.6923028345292646\n",
      "Training Loss at EPOCH 7225 ==> 0.6920367845015566\n",
      "Training Loss at EPOCH 7250 ==> 0.691772017421673\n",
      "Training Loss at EPOCH 7275 ==> 0.6915085244435141\n",
      "Training Loss at EPOCH 7300 ==> 0.6912462967904772\n",
      "Training Loss at EPOCH 7325 ==> 0.6909853257548907\n",
      "Training Loss at EPOCH 7350 ==> 0.6907256026974453\n",
      "Training Loss at EPOCH 7375 ==> 0.6904671190466375\n",
      "Training Loss at EPOCH 7400 ==> 0.6902098662982119\n",
      "Training Loss at EPOCH 7425 ==> 0.6899538360146106\n",
      "Training Loss at EPOCH 7450 ==> 0.689699019824427\n",
      "Training Loss at EPOCH 7475 ==> 0.6894454094218646\n",
      "Training Loss at EPOCH 7500 ==> 0.6891929965661989\n",
      "Training Loss at EPOCH 7525 ==> 0.6889417730812445\n",
      "Training Loss at EPOCH 7550 ==> 0.688691730854827\n",
      "Training Loss at EPOCH 7575 ==> 0.6884428618382591\n",
      "Training Loss at EPOCH 7600 ==> 0.6881951580458207\n",
      "Training Loss at EPOCH 7625 ==> 0.6879486115542427\n",
      "Training Loss at EPOCH 7650 ==> 0.6877032145021981\n",
      "Training Loss at EPOCH 7675 ==> 0.6874589590897923\n",
      "Training Loss at EPOCH 7700 ==> 0.6872158375780618\n",
      "Training Loss at EPOCH 7725 ==> 0.6869738422884769\n",
      "Training Loss at EPOCH 7750 ==> 0.6867329656024446\n",
      "Training Loss at EPOCH 7775 ==> 0.6864931999608208\n",
      "Training Loss at EPOCH 7800 ==> 0.6862545378634227\n",
      "Training Loss at EPOCH 7825 ==> 0.6860169718685466\n",
      "Training Loss at EPOCH 7850 ==> 0.6857804945924911\n",
      "Training Loss at EPOCH 7875 ==> 0.6855450987090813\n",
      "Training Loss at EPOCH 7900 ==> 0.6853107769491993\n",
      "Training Loss at EPOCH 7925 ==> 0.6850775221003168\n",
      "Training Loss at EPOCH 7950 ==> 0.6848453270060346\n",
      "Training Loss at EPOCH 7975 ==> 0.6846141845656224\n",
      "Training Loss at EPOCH 8000 ==> 0.6843840877335639\n",
      "Training Loss at EPOCH 8025 ==> 0.6841550295191063\n",
      "Training Loss at EPOCH 8050 ==> 0.683927002985813\n",
      "Training Loss at EPOCH 8075 ==> 0.6837000012511185\n",
      "Training Loss at EPOCH 8100 ==> 0.6834740174858911\n",
      "Training Loss at EPOCH 8125 ==> 0.6832490449139939\n",
      "Training Loss at EPOCH 8150 ==> 0.6830250768118531\n",
      "Training Loss at EPOCH 8175 ==> 0.6828021065080299\n",
      "Training Loss at EPOCH 8200 ==> 0.6825801273827934\n",
      "Training Loss at EPOCH 8225 ==> 0.6823591328676989\n",
      "Training Loss at EPOCH 8250 ==> 0.6821391164451694\n",
      "Training Loss at EPOCH 8275 ==> 0.6819200716480824\n",
      "Training Loss at EPOCH 8300 ==> 0.6817019920593542\n",
      "Training Loss at EPOCH 8325 ==> 0.6814848713115355\n",
      "Training Loss at EPOCH 8350 ==> 0.6812687030864046\n",
      "Training Loss at EPOCH 8375 ==> 0.6810534811145664\n",
      "Training Loss at EPOCH 8400 ==> 0.6808391991750548\n",
      "Training Loss at EPOCH 8425 ==> 0.6806258510949362\n",
      "Training Loss at EPOCH 8450 ==> 0.6804134307489196\n",
      "Training Loss at EPOCH 8475 ==> 0.6802019320589682\n",
      "Training Loss at EPOCH 8500 ==> 0.679991348993912\n",
      "Training Loss at EPOCH 8525 ==> 0.6797816755690697\n",
      "Training Loss at EPOCH 8550 ==> 0.6795729058458658\n",
      "Training Loss at EPOCH 8575 ==> 0.6793650339314583\n",
      "Training Loss at EPOCH 8600 ==> 0.6791580539783642\n",
      "Training Loss at EPOCH 8625 ==> 0.6789519601840905\n",
      "Training Loss at EPOCH 8650 ==> 0.6787467467907683\n",
      "Training Loss at EPOCH 8675 ==> 0.678542408084789\n",
      "Training Loss at EPOCH 8700 ==> 0.6783389383964445\n",
      "Training Loss at EPOCH 8725 ==> 0.6781363320995684\n",
      "Training Loss at EPOCH 8750 ==> 0.6779345836111839\n",
      "Training Loss at EPOCH 8775 ==> 0.67773368739115\n",
      "Training Loss at EPOCH 8800 ==> 0.6775336379418148\n",
      "Training Loss at EPOCH 8825 ==> 0.6773344298076687\n",
      "Training Loss at EPOCH 8850 ==> 0.6771360575750018\n",
      "Training Loss at EPOCH 8875 ==> 0.6769385158715644\n",
      "Training Loss at EPOCH 8900 ==> 0.6767417993662298\n",
      "Training Loss at EPOCH 8925 ==> 0.6765459027686588\n",
      "Training Loss at EPOCH 8950 ==> 0.6763508208289702\n",
      "Training Loss at EPOCH 8975 ==> 0.6761565483374098\n",
      "Training Loss at EPOCH 9000 ==> 0.6759630801240256\n",
      "Training Loss at EPOCH 9025 ==> 0.675770411058344\n",
      "Training Loss at EPOCH 9050 ==> 0.6755785360490492\n",
      "Training Loss at EPOCH 9075 ==> 0.6753874500436647\n",
      "Training Loss at EPOCH 9100 ==> 0.6751971480282383\n",
      "Training Loss at EPOCH 9125 ==> 0.6750076250270293\n",
      "Training Loss at EPOCH 9150 ==> 0.6748188761021975\n",
      "Training Loss at EPOCH 9175 ==> 0.6746308963534964\n",
      "Training Loss at EPOCH 9200 ==> 0.6744436809179676\n",
      "Training Loss at EPOCH 9225 ==> 0.6742572249696386\n",
      "Training Loss at EPOCH 9250 ==> 0.6740715237192217\n",
      "Training Loss at EPOCH 9275 ==> 0.6738865724138176\n",
      "Training Loss at EPOCH 9300 ==> 0.6737023663366191\n",
      "Training Loss at EPOCH 9325 ==> 0.6735189008066197\n",
      "Training Loss at EPOCH 9350 ==> 0.6733361711783217\n",
      "Training Loss at EPOCH 9375 ==> 0.6731541728414496\n",
      "Training Loss at EPOCH 9400 ==> 0.6729729012206642\n",
      "Training Loss at EPOCH 9425 ==> 0.6727923517752786\n",
      "Training Loss at EPOCH 9450 ==> 0.6726125199989796\n",
      "Training Loss at EPOCH 9475 ==> 0.6724334014195458\n",
      "Training Loss at EPOCH 9500 ==> 0.6722549915985748\n",
      "Training Loss at EPOCH 9525 ==> 0.6720772861312074\n",
      "Training Loss at EPOCH 9550 ==> 0.6719002806458559\n",
      "Training Loss at EPOCH 9575 ==> 0.671723970803936\n",
      "Training Loss at EPOCH 9600 ==> 0.671548352299598\n",
      "Training Loss at EPOCH 9625 ==> 0.6713734208594633\n",
      "Training Loss at EPOCH 9650 ==> 0.6711991722423605\n",
      "Training Loss at EPOCH 9675 ==> 0.6710256022390658\n",
      "Training Loss at EPOCH 9700 ==> 0.6708527066720438\n",
      "Training Loss at EPOCH 9725 ==> 0.6706804813951908\n",
      "Training Loss at EPOCH 9750 ==> 0.6705089222935824\n",
      "Training Loss at EPOCH 9775 ==> 0.6703380252832194\n",
      "Training Loss at EPOCH 9800 ==> 0.6701677863107791\n",
      "Training Loss at EPOCH 9825 ==> 0.6699982013533666\n",
      "Training Loss at EPOCH 9850 ==> 0.6698292664182696\n",
      "Training Loss at EPOCH 9875 ==> 0.6696609775427136\n",
      "Training Loss at EPOCH 9900 ==> 0.6694933307936212\n",
      "Training Loss at EPOCH 9925 ==> 0.6693263222673719\n",
      "Training Loss at EPOCH 9950 ==> 0.6691599480895628\n",
      "Training Loss at EPOCH 9975 ==> 0.6689942044147753\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X, y, train_steps = 10000, lr = 0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d64299d0-372a-4c5d-9184-a9aa2a258f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    dotptoduct = np.dot(a, b)\n",
    "    norm_dot = np.dot(np.linalg.norm(a), np.linalg.norm(b))\n",
    "    cosine = dotptoduct / norm_dot\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5c99c98-f25f-412d-8856-831e23174e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k = 1):\n",
    "    similarities = []\n",
    "    for c in candidates:\n",
    "        sim = cosine(v, c)\n",
    "        similarities.append(sim)\n",
    "\n",
    "    sorted_sim = np.argsort(similarities)\n",
    "    k_idx = sorted_sim[-k:]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f0f7f9c-c42f-4b20-9d50-a5d142c3d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57d5fbe0-1172-4899-9b29-b05741d48dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocab(X, Y, R):\n",
    "    k_idx = 0\n",
    "    pred = np.dot(X, R)\n",
    "    correct = 0\n",
    "    print(pred)\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(pred[i][:], Y, 1)\n",
    "        print(i, pred_idx)\n",
    "\n",
    "        if pred_idx == i:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / X.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f24874e8-66a9-424f-9465-ef93b0d9a690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12137843 0.06099942 0.07097218 0.05846171]\n",
      " [0.322052   0.26806939 0.28787949 0.15107235]\n",
      " [0.58393494 0.13825389 0.71333143 0.08430615]\n",
      " [0.0257261  0.02825108 0.05530665 0.0211075 ]]\n",
      "0 [3]\n",
      "1 [3]\n",
      "2 [2]\n",
      "3 [3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vocab(X, y, R_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9637e7bd-bab4-48db-a23e-aa9d7620b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5eb33163-a9f7-4d66-893c-333af2fcab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embedding):\n",
    "    doc_embedding = np.zeros(300)\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        doc_embedding += en_embedding.get(word, 0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7fcddbab-931f-49f0-83ae-33b8eeeb81e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings)\n",
    "tweet_embedding[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ad99cf80-70d6-403f-b0d4-020d53d35b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "    ind2doc_dict = {}\n",
    "    doc_vec_l = []\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        doc_emb = get_document_embedding(doc, en_embeddings)\n",
    "        ind2doc_dict[i] = doc_emb\n",
    "        doc_vec_l.append(doc_emb)\n",
    "    doc_vec_matrix = np.vstack(doc_vec_l)\n",
    "    return doc_vec_matrix, ind2doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4d0adb60-f62d-4bcd-9c3f-164d75c2b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "10169470-3d12-4d66-8dc8-a7603ba797c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(len(ind2Tweet))\n",
    "print(document_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1855edc2-61dc-4f88-b71a-398b079386f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"I'm sad\"\n",
    "processed = process_tweet(tweet)\n",
    "tweet_embedding = get_document_embedding(tweet, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ed070228-c6b9-4d03-b170-b572784b5cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5964\\3365568139.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  cosine = dotptoduct / norm_dot\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(cosine(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "34ae9924-c0d6-49ff-bf7c-ac5cf2e0cf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n",
    "N_PLANES = 10\n",
    "# Number of times to repeat the hashing to improve the search.\n",
    "N_UNIVERSES = 25\n",
    "\n",
    "N_VECS = len(all_tweets)       # This many vectors.\n",
    "N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9c851301-86f4-43f1-a6e2-fd2ef150754d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 10)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n",
    "            for _ in range(N_UNIVERSES)]\n",
    "planes_l[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d9abff56-37bb-4cca-afb7-0309ffd28f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    # step 1: dot product between the vector and the planes matrix\n",
    "    dotproduct = np.dot(v, planes)\n",
    "    # get the sign of each value\n",
    "    sign_of_dotproduct = np.sign(dotproduct)\n",
    "    # determine which sides each value belongs\n",
    "    h = np.where(sign_of_dotproduct > 0, 1, 0)\n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    \n",
    "    hash_value = 0\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        hash_value += 2 ** i * h[i]\n",
    "\n",
    "    return int(hash_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "943b8b4c-c919-48cf-8ddb-a17b63929025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash value for the vector is 768\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "idx = 0\n",
    "planes = planes_l[idx]\n",
    "v = np.random.rand(1, 300)\n",
    "hash_value = hash_value_of_vector(v, planes)\n",
    "print(f\"Hash value for the vector is {hash_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "752a3368-718a-4347-8490-149778feda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "    n_planes = planes.shape[1]\n",
    "    n_buckets = 2 ** n_planes\n",
    "    \n",
    "    hash_table = {i: [] for i in range(n_buckets)}\n",
    "    id_table = {i: [] for i in range(n_buckets)}\n",
    "    \n",
    "    for i, v in enumerate(vecs):\n",
    "        h_value = hash_value_of_vector(v, planes)\n",
    "        \n",
    "        hash_table[h_value].append(v)\n",
    "        id_table[h_value].append(i)\n",
    "        \n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4c621b78-506a-4119-8a7f-54b45d96b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 10000 document vectors\n",
      "The id table at key 0 has 10000\n",
      "The first 5 document indices stored at key 0 of are [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "planes = planes_l[0]\n",
    "vec = np.random.rand(1, 300)\n",
    "tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n",
    "\n",
    "print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n",
    "print(f\"The id table at key 0 has {len(tmp_id_table[0])}\")\n",
    "print(f\"The first 5 document indices stored at key 0 of are {tmp_id_table[0][0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "114a8590-82c5-4fd7-9400-ba071c6d25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working on Univgerse #0\n",
      "We are working on Univgerse #1\n",
      "We are working on Univgerse #2\n",
      "We are working on Univgerse #3\n",
      "We are working on Univgerse #4\n",
      "We are working on Univgerse #5\n",
      "We are working on Univgerse #6\n",
      "We are working on Univgerse #7\n",
      "We are working on Univgerse #8\n",
      "We are working on Univgerse #9\n",
      "We are working on Univgerse #10\n",
      "We are working on Univgerse #11\n",
      "We are working on Univgerse #12\n",
      "We are working on Univgerse #13\n",
      "We are working on Univgerse #14\n",
      "We are working on Univgerse #15\n",
      "We are working on Univgerse #16\n",
      "We are working on Univgerse #17\n",
      "We are working on Univgerse #18\n",
      "We are working on Univgerse #19\n",
      "We are working on Univgerse #20\n",
      "We are working on Univgerse #21\n",
      "We are working on Univgerse #22\n",
      "We are working on Univgerse #23\n",
      "We are working on Univgerse #24\n"
     ]
    }
   ],
   "source": [
    "hash_tables = []\n",
    "id_tables = []\n",
    "\n",
    "for u in range(N_UNIVERSES):\n",
    "    print(f\"We are working on Univgerse #{u}\")\n",
    "    planes = planes_l[u]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ed6912af-5c4f-4674-960f-39d0811de88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, k = 1, num_universe = N_UNIVERSES):\n",
    "    vecs_to_consider_l = list()\n",
    "    ids_to_consider_l = list()\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    \n",
    "    for u in range(num_universe):\n",
    "        planes = planes_l[u]\n",
    "        h_value = hash_value_of_vector(v, planes)\n",
    "        h_table = hash_tables[u]\n",
    "        document_vectors_l = hash_table[h_value]\n",
    "        id_table = id_tables[u]\n",
    "        new_ids_to_consider = id_table[h_value]\n",
    "        \n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "        \n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                doc_vec_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(doc_vec_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "                \n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "    \n",
    "    # convert the vecs to consider set to a list, then to a numpy array\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "    \n",
    "    # call nearest neighbors on the reduced list of candidate vectors\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "    \n",
    "    # Use the nearest neighbor index list as indices into the ids to consider\n",
    "    # create a list of nearest neighbors by the document ids\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "    \n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bac72f4e-4d6d-4b2d-aa9b-76545a896cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_vecs, ind2Tweet\n",
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "961d7d83-dc36-4541-86b6-0f0f843bc2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "Fast considering 9999 vecs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5964\\3365568139.py:4: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cosine = dotptoduct / norm_dot\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(\n",
    "    doc_id, vec_to_search, planes_l, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3ad845a8-445e-41d5-8e09-d0d713eabd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 3337\n",
      "document contents: Photo: randy9bis: Beautiful physique, shaved, uncut, and a tattoo: sexy boi !  :-) http://t.co/XcWxo5jVVY\n",
      "Nearest neighbor at document id 3330\n",
      "document contents: @Twytterina indeed! :)\n",
      "Nearest neighbor at document id 9999\n",
      "document contents: @eawoman As a Hull supporter I am expecting a misserable few weeks :-(\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
