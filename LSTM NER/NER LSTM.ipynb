{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb0a0ef-9142-4326-9edb-af2fd57c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import os\n",
    "import pandas as pd\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from utils_copy_1 import get_params, get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55233ffb-dc66-4984-92fc-81a47d25c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0\n",
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "\n",
      "Labels 0\n",
      "O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "\n",
      "    Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv(\"./ner_dataset.csv\", encoding = \"ISO-8859-1\")\n",
    "train_sentences = open(\"./data/small/train/sentences.txt\", \"r\").readline()\n",
    "train_labels = open(\"./data/small/train/labels.txt\", \"r\").readline()\n",
    "# exploring data\n",
    "print(f\"Sentence 0\\n{train_sentences}\")\n",
    "print()\n",
    "print(f\"Labels 0\\n{train_labels}\")\n",
    "print()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675ed132-bf89-4ed9-9154-86ba0c42e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18524170-6a42-43ed-bb5a-73ccc9103455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the' index: 9\n",
      "'PAD' index: 35180\n"
     ]
    }
   ],
   "source": [
    "print(f\"'the' index: {vocab['the']}\")\n",
    "print(f\"'PAD' index: {vocab['<PAD>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792c5926-a265-49f8-870c-e3ea0ecf30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 35181\n",
      "\n",
      "Train size: 33570\n",
      "Validation size: 7194\n",
      "Test size: 7194\n",
      "\n",
      "Train sentence 0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
      "Train labels 0: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
      "\n",
      "Val sentence 0: [1020, 68, 5092, 50, 9, 29845, 1677, 18327, 1033, 9, 4452, 13, 522, 29846, 45, 10314, 223, 6582, 21]\n",
      "Val labels 0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]\n",
      "\n",
      "Test sentence 0: [4046, 3007, 18, 3793, 2474, 7895, 93, 45, 1701, 32653, 3179, 93, 134, 19565, 5343, 740, 93, 13, 45, 19116, 4181, 1813, 21]\n",
      "Test labels 0: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Tags to be predicted\n",
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n",
      "\n",
      "Tags size: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print()\n",
    "print(f\"Train size: {t_size}\")\n",
    "print(f\"Validation size: {v_size}\")\n",
    "print(f\"Test size: {test_size}\")\n",
    "print()\n",
    "print(f\"Train sentence 0: {t_sentences[0]}\")\n",
    "print(f\"Train labels 0: {t_labels[0]}\")\n",
    "print()\n",
    "print(f\"Val sentence 0: {v_sentences[0]}\")\n",
    "print(f\"Val labels 0: {v_labels[0]}\")\n",
    "print()\n",
    "print(f\"Test sentence 0: {test_sentences[0]}\")\n",
    "print(f\"Test labels 0: {test_labels[0]}\")\n",
    "print()\n",
    "print(f\"Tags to be predicted\")\n",
    "print(tag_map)\n",
    "print()\n",
    "print(f\"Tags size: {len(tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78b726-e63d-43c9-bfb0-88f50a105b0a",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020ee0a9-b5db-4b09-9cca-2df9a56fbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generator function\n",
    "def data_generator(batch_size, x, y, pad, shuffle = False):\n",
    "    # count number of lines\n",
    "    num_lines = len(x)\n",
    "    \n",
    "    # create list of indexes\n",
    "    data_indexes = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle indexes if shuffle is True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(data_indexes)\n",
    "\n",
    "    # track loction of current x, y\n",
    "    cur_index = 0\n",
    "    \n",
    "    while True:\n",
    "        # temporal array with zeros holds location of batched data\n",
    "        buffer_x = [0] * batch_size\n",
    "        buffer_y = [0] * batch_size\n",
    "\n",
    "        # storing x[index : index + batch_size], y[index : index + batch_size] with padded version\n",
    "        max_length = 0\n",
    "        for i in range(batch_size):\n",
    "                \n",
    "            # check if cur_index exceeds num_lines\n",
    "            if cur_index >= num_lines:\n",
    "                cur_index = 0\n",
    "                # reshuffle indexes if shuffle set to True\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(data_indexes)\n",
    "                    \n",
    "            # store raw data from x in buffer_x,also with its corsponding y value\n",
    "            buffer_x[i] = x[data_indexes[cur_index]]\n",
    "            buffer_y[i] = y[data_indexes[cur_index]]\n",
    "            \n",
    "            # get the length of the current buffer_x[i]\n",
    "            len_x = len(buffer_x[i])\n",
    "            \n",
    "            # if the len_x greater than max_len set mx_length to len_x\n",
    "            if len_x > max_length:\n",
    "                max_length = len_x\n",
    "                \n",
    "            # increment index by 1\n",
    "            cur_index += 1\n",
    "            \n",
    "        # X, Y arrays of shape (batch_size, max_length) as padded values\n",
    "        X = np.full((batch_size, max_length), pad)\n",
    "        Y = np.full((batch_size, max_length), pad)\n",
    "        # fill X,Y with buffer_x, buffer_y lists\n",
    "        for i in range(batch_size):\n",
    "            x_i = buffer_x[i]\n",
    "            y_i = buffer_y[i]\n",
    "            # looping through each word in x_i\n",
    "            for j in range(len(x_i)):\n",
    "                X[i, j] = x_i[j]\n",
    "                Y[i, j] = y_i[j]\n",
    "        \n",
    "        # yield (X, Y)\n",
    "        yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e663437-b512-41c0-911e-2ae87f7fad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30) (5, 30) (5, 30) (5, 30)\n",
      "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14     9    15     1    16    17    18    19    20    21\n",
      " 35180 35180 35180 35180 35180 35180] \n",
      " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
      "     1     0     0     0     0     0     2     0     0     0     0     0\n",
      " 35180 35180 35180 35180 35180 35180]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels = t_labels[0: 8]\n",
    "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9fa958-8e8b-4848-b641-55b93a60c8ec",
   "metadata": {},
   "source": [
    "### Model architcure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ff4868-91f5-4d0d-bfac-95da4ec3c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(vocab_size = vocab_size, d_model = 128, tags = tag_map):\n",
    "    model = tl.Serial(\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        tl.LSTM(n_units = d_model),\n",
    "        tl.Dense(len(tags)),\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf04f7be-53f5-468b-81b8-d43c82d3d1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_35181_50\n",
       "  LSTM_50\n",
       "  Dense_17\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_model = LSTM(vocab_size = vocab_size, d_model = 50, tags = tag_map)\n",
    "display(tmp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de7441-cf3a-4794-946c-4d7a7f522598",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff57c68c-27f1-45cd-8a36-a384ad07cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "from trax.data import add_loss_weights\n",
    "\n",
    "\n",
    "def train_model(model, train_data, train_labels, eval_data, eval_labels, data_generator = data_generator, batch_size = 64, pad = vocab[\"<PAD>\"], n_steps = 1, shuffle = True, output_dir = \"model/\"):\n",
    "    # creating train_generator\n",
    "    train_generator = add_loss_weights(\n",
    "        data_generator(batch_size, train_data, train_labels, pad, True),\n",
    "        id_to_mask = pad)\n",
    "    \n",
    "    # creating eval generator\n",
    "    eval_generator = add_loss_weights(\n",
    "        data_generator(batch_size, eval_data, eval_labels, pad, True),\n",
    "        id_to_mask = pad)\n",
    "\n",
    "    # creating train task\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data = train_generator,\n",
    "        loss_layer = tl.CrossEntropyLoss(),\n",
    "        optimizer = trax.optimizers.Adam(0.001),\n",
    "        n_steps_per_checkpoint = 10\n",
    "    )\n",
    "    # creatng eval task\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data = eval_generator,\n",
    "        metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    "        n_eval_batches = 10\n",
    "    )\n",
    "    # define trainig loop\n",
    "    training_loop = training.Loop(\n",
    "        model,\n",
    "        train_task,\n",
    "        eval_tasks = [eval_task],\n",
    "        output_dir = output_dir\n",
    "    )\n",
    "    # run model for # epochs\n",
    "    training_loop.run(n_steps)\n",
    "    # return training_loop\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1ac0e2d-80b9-4757-bdfc-b0dc41e88658",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 350\n",
    "ner_model = LSTM(vocab_size = vocab_size, d_model = 128, tags = tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d3fad2d-51b9-4fa6-b5cb-a337b79abb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\NLP tutorial\\Deep neural net\\cleanenv\\lib\\site-packages\\jax\\_src\\xla_bridge.py:1053: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\Desktop\\NLP tutorial\\Deep neural net\\cleanenv\\lib\\site-packages\\trax\\layers\\base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
      "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 4636945\n",
      "Step      1: Ran 1 train steps in 16.07 secs\n",
      "Step      1: train CrossEntropyLoss |  2.36170220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\NLP tutorial\\Deep neural net\\cleanenv\\lib\\site-packages\\trax\\supervised\\training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
      "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step      1: eval  CrossEntropyLoss |  1.97816901\n",
      "Step      1: eval          Accuracy |  0.35201188\n",
      "\n",
      "Step     10: Ran 9 train steps in 67.00 secs\n",
      "Step     10: train CrossEntropyLoss |  1.18887854\n",
      "Step     10: eval  CrossEntropyLoss |  0.86785098\n",
      "Step     10: eval          Accuracy |  0.84948053\n",
      "\n",
      "Step     20: Ran 10 train steps in 42.77 secs\n",
      "Step     20: train CrossEntropyLoss |  0.86420357\n",
      "Step     20: eval  CrossEntropyLoss |  0.81392171\n",
      "Step     20: eval          Accuracy |  0.84810922\n",
      "\n",
      "Step     30: Ran 10 train steps in 18.63 secs\n",
      "Step     30: train CrossEntropyLoss |  0.80493563\n",
      "Step     30: eval  CrossEntropyLoss |  0.79448045\n",
      "Step     30: eval          Accuracy |  0.84451742\n",
      "\n",
      "Step     40: Ran 10 train steps in 30.73 secs\n",
      "Step     40: train CrossEntropyLoss |  0.77113956\n",
      "Step     40: eval  CrossEntropyLoss |  0.77103152\n",
      "Step     40: eval          Accuracy |  0.84634164\n",
      "\n",
      "Step     50: Ran 10 train steps in 15.75 secs\n",
      "Step     50: train CrossEntropyLoss |  0.76410991\n",
      "Step     50: eval  CrossEntropyLoss |  0.79012509\n",
      "Step     50: eval          Accuracy |  0.83553268\n",
      "\n",
      "Step     60: Ran 10 train steps in 75.60 secs\n",
      "Step     60: train CrossEntropyLoss |  0.75971669\n",
      "Step     60: eval  CrossEntropyLoss |  0.70361840\n",
      "Step     60: eval          Accuracy |  0.85234002\n",
      "\n",
      "Step     70: Ran 10 train steps in 29.85 secs\n",
      "Step     70: train CrossEntropyLoss |  0.72578847\n",
      "Step     70: eval  CrossEntropyLoss |  0.66498101\n",
      "Step     70: eval          Accuracy |  0.85391358\n",
      "\n",
      "Step     80: Ran 10 train steps in 27.61 secs\n",
      "Step     80: train CrossEntropyLoss |  0.67268908\n",
      "Step     80: eval  CrossEntropyLoss |  0.63263010\n",
      "Step     80: eval          Accuracy |  0.85316687\n",
      "\n",
      "Step     90: Ran 10 train steps in 20.40 secs\n",
      "Step     90: train CrossEntropyLoss |  0.63184601\n",
      "Step     90: eval  CrossEntropyLoss |  0.61368401\n",
      "Step     90: eval          Accuracy |  0.84592745\n",
      "\n",
      "Step    100: Ran 10 train steps in 26.63 secs\n",
      "Step    100: train CrossEntropyLoss |  0.57786000\n",
      "Step    100: eval  CrossEntropyLoss |  0.56202455\n",
      "Step    100: eval          Accuracy |  0.85604843\n",
      "\n",
      "Step    110: Ran 10 train steps in 20.68 secs\n",
      "Step    110: train CrossEntropyLoss |  0.55204636\n",
      "Step    110: eval  CrossEntropyLoss |  0.51407928\n",
      "Step    110: eval          Accuracy |  0.86785189\n",
      "\n",
      "Step    120: Ran 10 train steps in 18.57 secs\n",
      "Step    120: train CrossEntropyLoss |  0.52629578\n",
      "Step    120: eval  CrossEntropyLoss |  0.48956512\n",
      "Step    120: eval          Accuracy |  0.87376454\n",
      "\n",
      "Step    130: Ran 10 train steps in 18.02 secs\n",
      "Step    130: train CrossEntropyLoss |  0.47315669\n",
      "Step    130: eval  CrossEntropyLoss |  0.46449581\n",
      "Step    130: eval          Accuracy |  0.87189270\n",
      "\n",
      "Step    140: Ran 10 train steps in 21.14 secs\n",
      "Step    140: train CrossEntropyLoss |  0.46624488\n",
      "Step    140: eval  CrossEntropyLoss |  0.42994767\n",
      "Step    140: eval          Accuracy |  0.88500765\n",
      "\n",
      "Step    150: Ran 10 train steps in 24.49 secs\n",
      "Step    150: train CrossEntropyLoss |  0.41744089\n",
      "Step    150: eval  CrossEntropyLoss |  0.40944492\n",
      "Step    150: eval          Accuracy |  0.88646756\n",
      "\n",
      "Step    160: Ran 10 train steps in 115.89 secs\n",
      "Step    160: train CrossEntropyLoss |  0.39174038\n",
      "Step    160: eval  CrossEntropyLoss |  0.38670501\n",
      "Step    160: eval          Accuracy |  0.89642804\n",
      "\n",
      "Step    170: Ran 10 train steps in 18.80 secs\n",
      "Step    170: train CrossEntropyLoss |  0.39445359\n",
      "Step    170: eval  CrossEntropyLoss |  0.36421753\n",
      "Step    170: eval          Accuracy |  0.90385231\n",
      "\n",
      "Step    180: Ran 10 train steps in 13.09 secs\n",
      "Step    180: train CrossEntropyLoss |  0.34896746\n",
      "Step    180: eval  CrossEntropyLoss |  0.35835509\n",
      "Step    180: eval          Accuracy |  0.90236914\n",
      "\n",
      "Step    190: Ran 10 train steps in 19.86 secs\n",
      "Step    190: train CrossEntropyLoss |  0.34621316\n",
      "Step    190: eval  CrossEntropyLoss |  0.30913782\n",
      "Step    190: eval          Accuracy |  0.91954337\n",
      "\n",
      "Step    200: Ran 10 train steps in 21.61 secs\n",
      "Step    200: train CrossEntropyLoss |  0.32766426\n",
      "Step    200: eval  CrossEntropyLoss |  0.31590981\n",
      "Step    200: eval          Accuracy |  0.91926532\n",
      "\n",
      "Step    210: Ran 10 train steps in 14.23 secs\n",
      "Step    210: train CrossEntropyLoss |  0.30695757\n",
      "Step    210: eval  CrossEntropyLoss |  0.30548574\n",
      "Step    210: eval          Accuracy |  0.92099618\n",
      "\n",
      "Step    220: Ran 10 train steps in 20.51 secs\n",
      "Step    220: train CrossEntropyLoss |  0.30202585\n",
      "Step    220: eval  CrossEntropyLoss |  0.28476435\n",
      "Step    220: eval          Accuracy |  0.92812309\n",
      "\n",
      "Step    230: Ran 10 train steps in 15.20 secs\n",
      "Step    230: train CrossEntropyLoss |  0.28576389\n",
      "Step    230: eval  CrossEntropyLoss |  0.26126733\n",
      "Step    230: eval          Accuracy |  0.93320122\n",
      "\n",
      "Step    240: Ran 10 train steps in 10.87 secs\n",
      "Step    240: train CrossEntropyLoss |  0.26623386\n",
      "Step    240: eval  CrossEntropyLoss |  0.28354296\n",
      "Step    240: eval          Accuracy |  0.92761320\n",
      "\n",
      "Step    250: Ran 10 train steps in 11.31 secs\n",
      "Step    250: train CrossEntropyLoss |  0.26008746\n",
      "Step    250: eval  CrossEntropyLoss |  0.25778225\n",
      "Step    250: eval          Accuracy |  0.93628470\n",
      "\n",
      "Step    260: Ran 10 train steps in 12.59 secs\n",
      "Step    260: train CrossEntropyLoss |  0.26023936\n",
      "Step    260: eval  CrossEntropyLoss |  0.23962427\n",
      "Step    260: eval          Accuracy |  0.93807835\n",
      "\n",
      "Step    270: Ran 10 train steps in 13.04 secs\n",
      "Step    270: train CrossEntropyLoss |  0.23096208\n",
      "Step    270: eval  CrossEntropyLoss |  0.23982579\n",
      "Step    270: eval          Accuracy |  0.93548598\n",
      "\n",
      "Step    280: Ran 10 train steps in 15.25 secs\n",
      "Step    280: train CrossEntropyLoss |  0.22494438\n",
      "Step    280: eval  CrossEntropyLoss |  0.22818435\n",
      "Step    280: eval          Accuracy |  0.93826574\n",
      "\n",
      "Step    290: Ran 10 train steps in 11.56 secs\n",
      "Step    290: train CrossEntropyLoss |  0.21098706\n",
      "Step    290: eval  CrossEntropyLoss |  0.22751160\n",
      "Step    290: eval          Accuracy |  0.93956679\n",
      "\n",
      "Step    300: Ran 10 train steps in 10.77 secs\n",
      "Step    300: train CrossEntropyLoss |  0.22489972\n",
      "Step    300: eval  CrossEntropyLoss |  0.20932877\n",
      "Step    300: eval          Accuracy |  0.94448057\n",
      "\n",
      "Step    310: Ran 10 train steps in 11.27 secs\n",
      "Step    310: train CrossEntropyLoss |  0.21745352\n",
      "Step    310: eval  CrossEntropyLoss |  0.21595389\n",
      "Step    310: eval          Accuracy |  0.94361422\n",
      "\n",
      "Step    320: Ran 10 train steps in 11.34 secs\n",
      "Step    320: train CrossEntropyLoss |  0.21090409\n",
      "Step    320: eval  CrossEntropyLoss |  0.19186164\n",
      "Step    320: eval          Accuracy |  0.94770061\n",
      "\n",
      "Step    330: Ran 10 train steps in 13.15 secs\n",
      "Step    330: train CrossEntropyLoss |  0.20673954\n",
      "Step    330: eval  CrossEntropyLoss |  0.19853080\n",
      "Step    330: eval          Accuracy |  0.94544644\n",
      "\n",
      "Step    340: Ran 10 train steps in 10.31 secs\n",
      "Step    340: train CrossEntropyLoss |  0.19645020\n",
      "Step    340: eval  CrossEntropyLoss |  0.20289764\n",
      "Step    340: eval          Accuracy |  0.94426611\n",
      "\n",
      "Step    350: Ran 10 train steps in 9.65 secs\n",
      "Step    350: train CrossEntropyLoss |  0.19104674\n",
      "Step    350: eval  CrossEntropyLoss |  0.19256918\n",
      "Step    350: eval          Accuracy |  0.94720762\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "training_loop = train_model(ner_model, t_sentences, t_labels, v_sentences, v_labels, batch_size = BATCH_SIZE, n_steps = EPOCHS, output_dir = \"model_3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943e344-69ca-4cda-be6a-53f5321652c2",
   "metadata": {},
   "source": [
    "### Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a909c177-d87c-46aa-af87-4f8f31ea7d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7194, 70)\n",
      "(7194, 70)\n"
     ]
    }
   ],
   "source": [
    "test_gen = data_generator(len(test_sentences), test_sentences, test_labels, vocab[\"<PAD>\"], False)\n",
    "test_x, test_y = next(test_gen)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e62a5bd-ab74-4e50-b295-7bab28824cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_lstm(model_path, vocab_size = vocab_size, tag_map = tag_map, d_model=128, seq_len=10):\n",
    "    model = LSTM(vocab_size=vocab_size, d_model=d_model, tags=tag_map)\n",
    "    model.init(trax.shapes.ShapeDtype((1, seq_len), dtype=np.int32))\n",
    "    model.init_from_file(model_path, weights_only=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "779ba6d0-7b5f-4422-b4ef-e271a38de46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_lstm('./model_3/model.pkl.gz')\n",
    "\n",
    "# Initialize with a dummy input shape\n",
    "# model.init(trax.shapes.ShapeDtype((1, 10), dtype=np.int32))  # sequence length = 10\n",
    "\n",
    "# # Load trained weights\n",
    "# model.init_from_file('./model_3/model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1de7f57-6e64-421b-bf47-5615c9a4944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_35181_128\n",
       "  LSTM_128\n",
       "  Dense_17\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c81bbcd-9ca5-432d-8eae-090473e7a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7194, 70, 17)\n"
     ]
    }
   ],
   "source": [
    "# make dummy prediction\n",
    "tmp_preds = model(test_x)\n",
    "print(tmp_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17bc5eed-1fa5-4c56-bb4a-3915d6847e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7194, 70)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tmp_preds, axis = 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b293768-2415-41bc-b769-85990c62b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating model performance\n",
    "def evaluate_model(preds, labels, pad):\n",
    "    # choose max entity label index from axis 2\n",
    "    outputs = np.argmax(preds, axis = 2) # preds shape = (batch size, padded example length, entities)\n",
    "    print(f\"Outputs shape: {outputs.shape}\") # (batch size, padded example length)\n",
    "    # create mask of non padded sentences\n",
    "    mask = (labels != pad)\n",
    "    # get accuracy\n",
    "    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\n",
    "    # return accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a00f2a90-faf2-4f2d-9908-b122d278867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape: (7194, 70)\n",
      "Accuracy = 94.49607849121094%\n"
     ]
    }
   ],
   "source": [
    "# test evaluation function\n",
    "tmp_preds = model(test_x)\n",
    "accuracy = evaluate_model(tmp_preds, test_y, vocab[\"<PAD>\"])\n",
    "print(f\"Accuracy = {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1261abd-a1b5-412b-bd93-61fb5bd503f4",
   "metadata": {},
   "source": [
    "### Inference case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a07d568-a381-4a5f-8077-4188270967e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, model, vocab, tags):\n",
    "    # create a tensor of sentence\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sent.split(' ')]\n",
    "    # create batch of data\n",
    "    batch = np.ones((1, len(s)))\n",
    "    batch[0][:] = s\n",
    "    sentence = np.array(batch).astype(int)\n",
    "    # get output\n",
    "    output = model(sentence)\n",
    "    # get entities\n",
    "    outputs = np.argmax(output, axis = 2)\n",
    "    # get labels\n",
    "    labels = list(tags.keys())\n",
    "    # initialize empty list\n",
    "    pred = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i]\n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faa0886b-d9eb-468b-b8b7-06c23eda2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\n",
      "Tokenized sentence: [4921, 35179, 9, 2046, 2047, 4512, 1, 731, 13, 6716, 4313, 1, 35179, 172, 11, 134, 1470, 63, 350, 3525, 19, 9, 2046, 2047, 59, 2594, 7, 359, 223, 9, 8138, 1, 45, 103, 1003, 1, 9, 35179, 11, 9, 35179, 3415, 502, 172, 58, 35179, 21257, 1140]\n",
      "Named Entities: ['B-per', 'O', 'O', 'B-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O', 'O', 'B-org', 'I-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Peter B-per\n",
      "White B-org\n",
      "House I-org\n",
      "Sunday B-tim\n",
      "morning I-tim\n",
      "White B-org\n",
      "House I-org\n"
     ]
    }
   ],
   "source": [
    "sent = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sent.split(' ')]\n",
    "pred_labels = predict(sent, model, vocab, tag_map)\n",
    "print(f\"Sentence: {sent}\")\n",
    "print(f\"Tokenized sentence: {s}\")\n",
    "print(f\"Named Entities: {pred_labels}\")\n",
    "\n",
    "for x,y in zip(sent.split(' '), pred_labels):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (traxenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
